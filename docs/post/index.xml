<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on fml blog</title>
    <link>https://fml-fam.github.io/blog/post/</link>
    <description>Recent content in Posts on fml blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Mon, 25 Oct 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://fml-fam.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Computing Eigenvalues Out-of-Core</title>
      <link>https://fml-fam.github.io/blog/2021/10/25/computing-eigenvalues-out-of-core/</link>
      <pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2021/10/25/computing-eigenvalues-out-of-core/</guid>
      <description>&lt;p&gt;In an &lt;a href=&#34;https://fml-fam.github.io/blog/2020/06/15/svd-via-lanczos-iteration/&#34;&gt;earlier post&lt;/a&gt;, I discussed how you can compute SVD (and spectral decomposition) via Lanczos Iteration. Several times in that post, I mentioned that the method lends itself well to out-of-core data &amp;mdash; that is, data stored on disk rather than in memory. But I never bothered to actually go into any details. Recently I had the need for an out-of-core eigensolver, and I have implemented it in &lt;a href=&#34;https://github.com/wrathematics/hdfmat&#34;&gt;this R package&lt;/a&gt;. But I would like to describe the details, because assuming you understand the Lanczos part from the earlier article, then this really isn&amp;rsquo;t too complicated.&lt;/p&gt;
&lt;p&gt;I would say that the hard part is working out-of-core. There are many choices for something like this. You can go with some kind of data standard like HDF5 (which is what I did), &amp;ldquo;roll your own&amp;rdquo; with something like &lt;code&gt;fwrite()&lt;/code&gt;, or split the difference with some kind of binary xml monstrosity. I&amp;rsquo;m not an I/O specialist so I went the route I felt was easiest, even if it cost me some performance. It&amp;rsquo;s also worth mentioning that I sort of feel like once you make the compromise of working out-of-core that being overly concerned with performance is a bit silly. If you have a giant dataset and performance matters, assuming you&amp;rsquo;re working in open science, you should get access to a compute cluster. There are tons of free resources available.&lt;/p&gt;
&lt;p&gt;For each Lanczos iteration $i$ from $1, \dots, k$ do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute $v$ scalar by scalar, iterating over the rows $j$ of $A$, with $j = 1, \dots, n$
&lt;ol&gt;
&lt;li&gt;Read row $j$ of $A$, denoted $A_j$&lt;/li&gt;
&lt;li&gt;Denote the $i$&#39;th column of $Q$ by $Q^T_i$&lt;/li&gt;
&lt;li&gt;Compute the $j$&#39;th element of $v$ (a scalar) as the dot product $v_j = A_j \cdot Q^T_i$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Proceed as before calculating $alpha$, $v$ updates, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some of the formalism obscures how easy this really is. Here it makes the most sense to store $Q$ in column-major storage and $A$ on disk in row-major format. Then you just have to shift your $Q$ array by $n\times i \times \texttt{sizeof}(Q)$ bytes.&lt;/p&gt;
&lt;p&gt;Of course, you don&amp;rsquo;t necessarily have to read rows. You can read whatever blocks you want that your out-of-core storage supports. The bigger the read, the more data you can operate on at a time, which will probably improve the run-time performance. I chose rows because it&amp;rsquo;s easy to understand and implement. See also the above caveats about working out-of-core.&lt;/p&gt;
&lt;p&gt;With an R-like pseudo-code, it might look something like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# allocate/initialize alpha, beta, q as before&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(i in &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;k)
{
  &lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(j in &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;n)
  {
    A_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;read_row&lt;/span&gt;(row_offset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;j)
    v[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A_j &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; q[, i]
  }
  
  &lt;span style=&#34;color:#75715e&#34;&gt;# continue as before&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I have implemented this and a few other operations useful for computing shrink correlation in the aforementioned &lt;a href=&#34;https://github.com/wrathematics/hdfmat&#34;&gt;hdfmat package&lt;/a&gt;. My primary motivation was to make it easy to compute the eigenvalues of large shrink correlation matrices out-of-core, which actually dovetails with another &lt;a href=&#34;https://fml-fam.github.io/blog/2021/06/29/matrix-computations-in-constrained-memory-environments/&#34;&gt;earlier post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The hdfmat package uses HDF5 as the on-disk matrix container. As with basically anything, there are some tradeoffs here. HDF5 is a standard in HPC, so I was already familiar with it. If performance is critical, you could probably do better. But this really is the hill I will die on: if performance is so critical, then why are you working out-of-core in the first place?&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a little benchmark of the package. I used 8 cores of an 8-core AMD processor, and the disk is some garbage SSD that will probably die in 6 months. OpenBLAS is doing most of the hard work. The benchmark computes the crossproduct matrix of a &amp;ldquo;large&amp;rdquo;-ish matrix and then computes various numbers of estimates of the eigenvalues. These are basically the bottlenecks for the shrink correlation thing I mentioned earlier.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(hdfmat)
&lt;span style=&#34;color:#a6e22e&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt;)

f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tempfile&lt;/span&gt;()
name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mydata&amp;#34;&lt;/span&gt;
type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;float&amp;#34;&lt;/span&gt;

m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;
n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100000&lt;/span&gt;

&lt;span style=&#34;color:#a6e22e&#34;&gt;system.time&lt;/span&gt;({
  x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;rnorm&lt;/span&gt;(m&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n), m, n)
})
&lt;span style=&#34;color:#75715e&#34;&gt;##    user  system elapsed &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##   4.245   0.294   4.539 &lt;/span&gt;

&lt;span style=&#34;color:#a6e22e&#34;&gt;system.time&lt;/span&gt;({
  h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hdfmat&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;hdfmat&lt;/span&gt;(f, name, n, n, type)
})
&lt;span style=&#34;color:#75715e&#34;&gt;##    user  system elapsed &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##   0.003   0.007   0.011 &lt;/span&gt;

&lt;span style=&#34;color:#a6e22e&#34;&gt;system.time&lt;/span&gt;({
  h&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;crossprod&lt;/span&gt;(x)
})
&lt;span style=&#34;color:#75715e&#34;&gt;##      user    system   elapsed &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## 25721.003  1227.089  3375.184 &lt;/span&gt;

&lt;span style=&#34;color:#a6e22e&#34;&gt;system.time&lt;/span&gt;({
  h&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;eigen&lt;/span&gt;(k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
})
&lt;span style=&#34;color:#75715e&#34;&gt;##    user  system elapsed &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## 196.455  21.059  27.211 &lt;/span&gt;

&lt;span style=&#34;color:#a6e22e&#34;&gt;system.time&lt;/span&gt;({
  h&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;eigen&lt;/span&gt;(k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)
})
&lt;span style=&#34;color:#75715e&#34;&gt;##     user   system  elapsed &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## 6149.752  641.622  849.200 &lt;/span&gt;

&lt;span style=&#34;color:#a6e22e&#34;&gt;system.time&lt;/span&gt;({
  h&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;eigen&lt;/span&gt;(k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;)
})
&lt;span style=&#34;color:#75715e&#34;&gt;##      user    system   elapsed &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## 61362.140  6363.350  8468.052 &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When I check the resulting file with &lt;code&gt;du -h&lt;/code&gt;, I see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;38G	rdata.h5
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Which is reasonable given the size to store it in-memory is pretty close to that:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;memuse&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;howbig&lt;/span&gt;(n, n, prefix&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;SI&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## 40.000 GB&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Amusingly, I have enough memory to store that matrix on this workstation, but not enough to store it and compute the eigenvalues with a call to something like &lt;code&gt;eigen()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;memuse&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;Sys.meminfo&lt;/span&gt;()
&lt;span style=&#34;color:#75715e&#34;&gt;## Totalram:  62.814 GiB &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Freeram:   50.744 GiB &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That&amp;rsquo;s because &lt;code&gt;eigen()&lt;/code&gt; will make a copy of the matrix, and I only have enough room for one on this workstation. However, I can actually compute this using &lt;a href=&#34;https://github.com/fml-fam/fmlr&#34;&gt;fmlr&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;suppressMessages&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(fmlr))
&lt;span style=&#34;color:#a6e22e&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt;)

m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;
n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100000&lt;/span&gt;

&lt;span style=&#34;color:#a6e22e&#34;&gt;system.time&lt;/span&gt;({
  x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cpumat&lt;/span&gt;(m, n, type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;float&amp;#34;&lt;/span&gt;)
  x&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;fill_rnorm&lt;/span&gt;()
})
&lt;span style=&#34;color:#75715e&#34;&gt;##  user  system elapsed &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## 1.519   0.045   1.565 &lt;/span&gt;

&lt;span style=&#34;color:#a6e22e&#34;&gt;system.time&lt;/span&gt;({
  cp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;linalg_crossprod&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, x)
})
&lt;span style=&#34;color:#75715e&#34;&gt;##    user  system elapsed &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## 212.216  10.629  36.144 &lt;/span&gt;

&lt;span style=&#34;color:#a6e22e&#34;&gt;system.time&lt;/span&gt;({
  ev &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cpuvec&lt;/span&gt;(type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;float&amp;#34;&lt;/span&gt;)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;linalg_eigen_sym&lt;/span&gt;(x, ev)
})
&lt;span style=&#34;color:#75715e&#34;&gt;##   user  system elapsed &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## 40.442   9.524   6.720&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Which is just &lt;em&gt;a bit&lt;/em&gt; faster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Bit About Checkpoint/Restart</title>
      <link>https://fml-fam.github.io/blog/2021/09/09/a-bit-about-checkpoint/restart/</link>
      <pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2021/09/09/a-bit-about-checkpoint/restart/</guid>
      <description>&lt;p&gt;Checkpoint/Restart (C/R) is a fault tolerance strategy common in high performance computing, but as far as I can tell, almost totally unknown to statistical computing. The basic idea is that you save some/all of the state from your running program in order to be able to resume it in the event that the computation is interrupted.&lt;/p&gt;
&lt;p&gt;Maybe it&amp;rsquo;s storming outside and your power is spotty. Even if you have a laptop or a power supply, how long will that downed tree knock out your power? Perhaps a more typical problem is that you are working on a shared computing resource, like a campus cluster or even a large HPC machine. These kinds of resources typically operate with fixed wall clock run windows. Maybe that window is 24 hours, and you aren&amp;rsquo;t sure that your job will finish in that time. In that case, what do you do? You can potentially go parallel (multicore and/or multi-node) or otherwise make your job run faster. Or maybe you don&amp;rsquo;t feel like doing that for some reason. I&amp;rsquo;m not here to judge you, friend.&lt;/p&gt;
&lt;p&gt;In cases like this, C/R is a good strategy to make sure that your job actually completes some day. But before we get into the details, let&amp;rsquo;s focus our attention on a specific kind of computation. Because if your task runs in 5 seconds, what exactly are you worried about? So probably this is something that is at least potentially long running. And based on my experience supporting users with big statistical compute problems, the majority of these are task-based. So imagine you have many &amp;ldquo;small&amp;rdquo; operations you want to run, rather than one single &amp;ldquo;large&amp;rdquo; operation (what exactly these things mean is a bit ambiguous &amp;ndash; just bear with me).&lt;/p&gt;
&lt;p&gt;Because my target audience is statistical computing folks, I will be giving examples in R. But obviously these concepts apply to every language. For running many-task things in R, you could use &lt;code&gt;lapply()&lt;/code&gt; or its many clones. If you want to go parallel, there&amp;rsquo;s the built-in parallel package, which is great. If you want some kind of different interface, I like the &lt;a href=&#34;https://cran.r-project.org/web/packages/future/index.html&#34;&gt;future package&lt;/a&gt;. If you want to go distributed, I think the &lt;a href=&#34;https://cran.r-project.org/web/packages/pbdMPI/index.html&#34;&gt;pbdMPI package&lt;/a&gt; (of which I am a co-author) offers some unique advantages. And there are about 100 other such packages, which all have pros and cons, but all do similar things. And although I will mention some packages I have written at the tail of this article, the point here is not to shill my software. My hope is that more packages implement something like this.&lt;/p&gt;
&lt;p&gt;Ok, so we&amp;rsquo;re going to be saving the state of some kind of computation. We can serialize with R&amp;rsquo;s &lt;code&gt;save()&lt;/code&gt; and &lt;code&gt;load()&lt;/code&gt;. You may be able to make the I/O faster with some of the recent, boutique packages for serializing R data. And for some applications, this may be very important. But this approach is dependency-free, which I feel is an advantage which often goes underappreciated in the R community.&lt;/p&gt;
&lt;p&gt;There are some caveats worth discussing where this strategy ranges from difficult to impossible to implement. Again, we&amp;rsquo;re focusing for the moment on task parallelism (because that&amp;rsquo;s the easy problem). But let&amp;rsquo;s say you have some really long running, single function evaluation - like some really big matrix operation, fitting a linear model, etc. In this case there may not be much you can do unless you wrote the thing yourself. Some encapsulated compiled code that you call can&amp;rsquo;t really be intercepted mid-evaluation. You may be able to save/load intermittent parts of your overall pipeline, but that&amp;rsquo;s much more ad hoc than what I want to talk about here.&lt;/p&gt;
&lt;p&gt;Another caveat is that some package do custom memory allocation, which R calls &amp;ldquo;external pointers&amp;rdquo;. R does not actually understand these objects; it merely pretends to. Any package that uses these &amp;ndash; for example, the &lt;a href=&#34;https://github.com/fml-fam/fmlr&#34;&gt;fmlr package&lt;/a&gt;, but there are many others &amp;ndash; can&amp;rsquo;t be used for C/R.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re finally ready to talk about implementing a C/R strategy for a large, many-task problem. The problem naturally splits into a few distinct pieces:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 0: Notation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will mimic the notation of &lt;code&gt;lapply()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt; is our data, some sort of list/vector object&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FUN&lt;/code&gt; is the function we want to apply to the data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;...&lt;/code&gt; are the additional arguments to &lt;code&gt;FUN&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some other values we will use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt; is the number of items (&lt;code&gt;length(X)&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;checkpoint_file&lt;/code&gt; is the file we will save our checkpoint to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;checkpoint_freq&lt;/code&gt; is the frequency of checkpoint writing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we have &lt;code&gt;checkpoint_freq=1&lt;/code&gt; then every time a function evaluation completes, we write out to disk. With &lt;code&gt;checkpoint_freq=2&lt;/code&gt;, then we write out every other time. This is to balance the cost of computation vs the cost of I/O. This will be somewhat application dependent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1: Initialize or Restart&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s denote &lt;code&gt;start&lt;/code&gt; as the index of the first element yet to be evaluated. If we have not yet started computing, then &lt;code&gt;start=1&lt;/code&gt;. If we have evaluated 10 items, then &lt;code&gt;start=11&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;At the beginning of the workflow, we need to see if we are restarting or if we are just starting:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;file.exists&lt;/span&gt;(checkpoint_file))
  &lt;span style=&#34;color:#a6e22e&#34;&gt;load&lt;/span&gt;(file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;checkpoint_file)
else
{
  start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1L&lt;/span&gt;
  ret &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;vector&lt;/span&gt;(length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;n, mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;list&amp;#34;&lt;/span&gt;)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first block merely loads the checkpoint file if we are restarting. The second allocates space for the return object and initializes the index.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2: Evaluate&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now for the real work. We need to iterate through the indices applying the function and checkpointing as necessary:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(i in start&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;n)
{
  ret[[i]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;FUN&lt;/span&gt;(X[i], &lt;span style=&#34;color:#66d9ef&#34;&gt;...&lt;/span&gt;)
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(n &lt;span style=&#34;color:#f92672&#34;&gt;%%&lt;/span&gt; checkpoint_freq &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
  {
    start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1L&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;save&lt;/span&gt;(start, ret, file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;checkpoint_file)
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you understand the R language and have followed so far, I feel like this is fairly straightforward. If you feel like &amp;ldquo;this isn&amp;rsquo;t really that complicated&amp;rdquo;, then I agree and you probably get it.&lt;/p&gt;
&lt;p&gt;As a final wrapup, you can remove the checkpoint file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;file.remove&lt;/span&gt;(checkpoint_file)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The net effect of all of this is that &lt;code&gt;ret&lt;/code&gt; contains the output that it would if you had run &lt;code&gt;ret = lapply(X, FUN)&lt;/code&gt;. There is some minor overhead from bookkeeping. There is potentially major overhead from writing out the checkpoint depending on what is actually being stored in &lt;code&gt;ret&lt;/code&gt;. Using a faster serialization method may be helpful, as mentioned before. Another option would be trying to overlap the compute and the I/O using something like &lt;code&gt;parallel::mcparallel()&lt;/code&gt; and &lt;code&gt;parallel::mccollect()&lt;/code&gt;. Note that this is potentially dangerous.&lt;/p&gt;
&lt;p&gt;All of the above is encapsulated in the &lt;a href=&#34;https://github.com/wrathematics/crlapply&#34;&gt;crlapply package&lt;/a&gt;. The package is available on the &lt;a href=&#34;https://hpcran.org/&#34;&gt;HPCRAN&lt;/a&gt; and can be installed via&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;install.packages&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;crlapply&amp;#34;&lt;/span&gt;, repos&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://hpcran.org&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Pilfering the example from the package README, we can see how this actually works in practice. We start with an &amp;ldquo;expensive&amp;rdquo; function. Here all it does is sleep for a bit before returning a square root:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;costly &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(x, waittime)
{
  &lt;span style=&#34;color:#a6e22e&#34;&gt;Sys.sleep&lt;/span&gt;(waittime)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;paste&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration:&amp;#34;&lt;/span&gt;, x))
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(x)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using the C/R version of &lt;code&gt;lapply()&lt;/code&gt; available in the crlapply package, we can evaluate this like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;ret &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; crlapply&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;crlapply&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, costly, checkpoint_file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/tmp/cr.rdata&amp;#34;&lt;/span&gt;, waittime&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)

&lt;span style=&#34;color:#a6e22e&#34;&gt;unlist&lt;/span&gt;(ret)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Say we save this in the file &lt;code&gt;example.r&lt;/code&gt;. We can run this in batch and kill it a few times (the printed &lt;code&gt;^C&lt;/code&gt; represents Ctrl+C which kills the process).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ Rscript example.r
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 1&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 2&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 3&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 4&amp;#34;&lt;/span&gt;
^C
$ Rscript example.r
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 5&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 6&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 7&amp;#34;&lt;/span&gt;
^C
$ Rscript example.r
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 8&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 9&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iteration: 10&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice that indeed, each time we restart the script, it picks up right where it left off. The final line of the script, when executed, will produce the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;unlist(ret)
##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751
##  [8] 2.828427 3.000000 3.162278
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So what about parallelism? I have gotten a suprising amount of mileage out of the &lt;a href=&#34;https://github.com/RBigData/tasktools&#34;&gt;tasktools package&lt;/a&gt;, which uses MPI to solve this problem. The package is available on the &lt;a href=&#34;https://hpcran.org/&#34;&gt;HPCRAN&lt;/a&gt; and assuming you have a system installation of MPI available, it can be installed via&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;install.packages&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tasktools&amp;#34;&lt;/span&gt;, repos&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://hpcran.org&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://cran.rstudio.com&amp;#34;&lt;/span&gt;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It is meant to be used in batch with &lt;a href=&#34;https://en.wikipedia.org/wiki/SPMD&#34;&gt;SPMD style programming&lt;/a&gt;. I don&amp;rsquo;t really want to get into what this means right now, but I may in a future post. My hope is that others will begin utilizing these strategies in their various wrappers around &lt;code&gt;lapply()&lt;/code&gt; and &lt;code&gt;parallel::mclapply()&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Matrix Computations in Constrained Memory Environments</title>
      <link>https://fml-fam.github.io/blog/2021/06/29/matrix-computations-in-constrained-memory-environments/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2021/06/29/matrix-computations-in-constrained-memory-environments/</guid>
      <description>&lt;p&gt;Many times in my professional career, a researcher has approached me for help performing some kind of matrix calculation that is getting &amp;ldquo;expensive&amp;rdquo; as their data grows. Sometimes the expense is in compute time: it&amp;rsquo;s just taking too long. Recently, someone approached me who didn&amp;rsquo;t care at all how long it took to compute something. Their problem was getting the thing to run within the RAM constraints in their particular computing environment, which they couldn&amp;rsquo;t really deviate from.&lt;/p&gt;
&lt;p&gt;This particular researcher was using R, which shows remarkably good taste on their part. And although I will be using R for the code snippets, the general principles apply to any language.&lt;/p&gt;
&lt;p&gt;The actual problem is somewhat standard fare for someone coming from statistics. They have a few different data sets of varying sizes &amp;mdash; I don&amp;rsquo;t want to give exact numbers, but something on the order of 100,000 rows, and a varying number of columns, order of 10 at the small end and 1000 at the large end. They were interested in computing a correlation matrix based on the rows rather than the columns; i.e. they wanted to scale the data by mean and variance to produce a matrix $S$ and then compute&lt;/p&gt;
&lt;p&gt;$$ SS^T $$&lt;/p&gt;
&lt;p&gt;At first I didn&amp;rsquo;t blush at this because sometimes statisticians need to calculate correlation or covariance matrices to operate on their explicit values. That turns out to &lt;em&gt;not&lt;/em&gt; be what this researcher actually needed. Some readers may already see the punchline coming. For those who don&amp;rsquo;t, this turns out to make for a remarkably good demonstration of some basic computational principles, so let us proceed with the problem as stated.&lt;/p&gt;
&lt;p&gt;One option is to go out of core. You can &amp;ldquo;roll your own&amp;rdquo; with something like a SQL database or using HDF5. But there are some R packages specifically for this kind of thing, like &lt;a href=&#34;https://cran.r-project.org/web/packages/ff/index.html&#34;&gt;ff&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/bigmemory/index.html&#34;&gt;bigmemory&lt;/a&gt;, although I have no direct experience with either of these and can&amp;rsquo;t speak to how they handle numerics or their ease of use. Another option is to go distributed. For that, your major options are &lt;a href=&#34;https://pbdr.org/&#34;&gt;pbdR&lt;/a&gt; and &lt;a href=&#34;http://github.com/fml-fam/fmlr&#34;&gt;fmlr&lt;/a&gt;, the latter with the MPIMAT backend.&lt;/p&gt;
&lt;p&gt;These approaches have different pros and cons. Pro of working out of core is you can stick with your workstation. A major con is that you&amp;rsquo;re now reliant on the I/O performance of your disk, which sucks (yes, even your ssd). This can also be hard to actually implement (the aforementioned packages may make this much easier; again, I&amp;rsquo;ve never used them). On the other hand, a pro of going distributed is that it&amp;rsquo;s fast. The major con is that it&amp;rsquo;s hard to use; you have to move to batch computing, for example, which is unusal for a lot of R programmers. Another con is that you actually need access to multi-node compute resources. And given that I have spent the last decade working in supercomputing, I sometimes forget that not everyone has the kinds of resources lying around that I do.&lt;/p&gt;
&lt;p&gt;The researcher wasn&amp;rsquo;t excited about either option: we couldn&amp;rsquo;t move to a new resource, and they didn&amp;rsquo;t want to orchestrate an entirely new workflow. But there is another possibility: reducing precision from double to float (64-bit to 32-bit). This will basically cut the problem size in half, at the cost of potentially worse numerical estimates (depending on how many decimal places you actually need). My belief is that for statistics in particular, float is almost always good enough (fight me). At any rate, this made the researcher&amp;rsquo;s problem just barely fit in memory and seemed like a good enough compromise.&lt;/p&gt;
&lt;p&gt;For this, we can use the &lt;a href=&#34;https://cran.r-project.org/web/packages/float/index.html&#34;&gt;float package&lt;/a&gt;. When memory is really tight (as in this case), we need to be conscious about our copies. Here the dataset isn&amp;rsquo;t the major memory consumer, but it&amp;rsquo;s just big enough to be annoying when we need to compute the final crossproducts matrix. So we&amp;rsquo;ll be dropping objects as we go. And throughout, we&amp;rsquo;ll assume you have your data stored in the object named &lt;code&gt;df&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So this is a little ugly, but we can do this as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(float)

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;as.matrix&lt;/span&gt;(df)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(df)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

x_fl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fl&lt;/span&gt;(x)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(x)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;scale&lt;/span&gt;(x_fl, &lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;)
co &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tcrossprod&lt;/span&gt;(s) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fl&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;max&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1L&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(x)&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And voila, a correlation matrix.&lt;/p&gt;
&lt;p&gt;But unfortunately, that wasn&amp;rsquo;t good enough. Because although this worked, what the researcher &lt;em&gt;really&lt;/em&gt; wanted was the eigenvalues of this matrix. Yeah, that&amp;rsquo;s a problem, because R will always operate on copies when data has to be modified. And when you give R&amp;rsquo;s &lt;code&gt;eigen()&lt;/code&gt; a symmetric matrix (as is the case here), it will eventually call out to LAPACK&amp;rsquo;s &lt;code&gt;syevr()&lt;/code&gt; function (specifically, &lt;code&gt;ssyevr()&lt;/code&gt; in this case). That modifies the data, so R will try to copy that giant matrix that we could only barely fit in memory.&lt;/p&gt;
&lt;p&gt;Now if you insist on this route for some reason, you can actually do this with fmlr. This is because fmlr (and the core C++ library fml) is designed to live just barely above the underlying linear algebra frameworks. It&amp;rsquo;s high level enough to be easy to use, but low level enough to be performant in a way that basically no other library that I&amp;rsquo;m aware of can credibly claim to be (fight me).&lt;/p&gt;
&lt;p&gt;fmlr will not make modifications to data in a copy &amp;mdash; if you don&amp;rsquo;t like that, make your own damn copy. So this isn&amp;rsquo;t going to win any beauty pageants, but it works:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(fmlr)

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;as.matrix&lt;/span&gt;(df)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(df)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

x_fl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fl&lt;/span&gt;(x)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(x)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;as_cpumat&lt;/span&gt;(x_fl, copy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FALSE&lt;/span&gt;)

&lt;span style=&#34;color:#a6e22e&#34;&gt;dimops_scale&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;, s)
alpha &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, s&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;nrows&lt;/span&gt;()&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;)
co &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;linalg_tcrossprod&lt;/span&gt;(alpha, s)

&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(s)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(x_fl)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

vals_fl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cpuvec&lt;/span&gt;(type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;float&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;linalg_eigen_sym&lt;/span&gt;(co, vals_fl)
vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dbl&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(vals_fl&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;to_robj&lt;/span&gt;()))
vals
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Yeah I get it, wouldn&amp;rsquo;t it be nice if we could just do &lt;code&gt;eigen(tcrossprod(x))&lt;/code&gt;. That sure is a lot less typing. But the whole point is that we can&amp;rsquo;t do that.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Or can we?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In an &lt;a href=&#34;https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/&#34;&gt;earlier post&lt;/a&gt; I outlined the details of the well-known connection between the square of the singular values of a matrix and the eigenvalues its derived crossproducts matrices. I won&amp;rsquo;t go through it all again here, but basically we can solve the problem like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;svd&lt;/span&gt;(df, nu&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, nv&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;d)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And this can be done pretty quickly computationally, and it doesn&amp;rsquo;t really take all that much memory. The cost is mostly in two copies of the data: one going from the dataframe to a numeric matrix, and the second created before the data is modified by the LAPACK &lt;code&gt;dgesdd()&lt;/code&gt; driver. There is some additional overhead for workspace for the LAPACK function, but it&amp;rsquo;s not substantial.&lt;/p&gt;
&lt;p&gt;But if you were still highly memory constrained, you could get by with a single copy, and push the calculation down to float as before:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(float)

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;as.matrix&lt;/span&gt;(df)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(df)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

x_fl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fl&lt;/span&gt;(x)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(x)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

&lt;span style=&#34;color:#a6e22e&#34;&gt;dbl&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;svd&lt;/span&gt;(x_fl, nu&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, nv&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;d))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And if even that is pushing the limits, you can avoid the extra copy using fmlr as before:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(fmlr)

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;as.matrix&lt;/span&gt;(df)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(df)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

x_fl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fl&lt;/span&gt;(x)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(x)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;as_cpumat&lt;/span&gt;(x_fl, copy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FALSE&lt;/span&gt;)

v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cpuvec&lt;/span&gt;(type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;float&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;linalg_svd&lt;/span&gt;(s, v)

&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(s)
&lt;span style=&#34;color:#a6e22e&#34;&gt;rm&lt;/span&gt;(x_fl)
&lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gc&lt;/span&gt;())

&lt;span style=&#34;color:#a6e22e&#34;&gt;dbl&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(v&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;to_robj&lt;/span&gt;()))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Matrix Factorizations for Data Analysis</title>
      <link>https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/</link>
      <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/</guid>
      <description>&lt;p&gt;Integers can be factored into products of special kinds of integers with useful properties called primes. Similarly, matrices can be factored into products of special kinds of matrices with useful properties.&lt;/p&gt;
&lt;p&gt;Because data analysis often involves operating on numeric matrices, understanding how and why to factor matrices can be very helpful. We&amp;rsquo;re going to talk about the major factorizations, namely LU, Cholesky, QR, SVD, and Eigendecomposition. There are others, but if you master these, then you&amp;rsquo;re off to a great start.&lt;/p&gt;
&lt;p&gt;Throughout we&amp;rsquo;re going to assume that all matrices are real-valued, because it makes the math slightly more pleasant most of the time, and because statisticians would start flipping over tables if you gave them matrices with complex numbers. Most of the core ideas are the same; only the small details change for the most part &amp;mdash; replace transposes with hermitians and so on. If you work with complex numbers, you know the drill.&lt;/p&gt;
&lt;p&gt;This is written for the person who knows what a matrix is, has done some data analysis, and wants to know more about the math and computation behind many of the core operations of the field.&lt;/p&gt;
&lt;h2 id=&#34;lu&#34;&gt;LU&lt;/h2&gt;
&lt;p&gt;Anyone who took a matrix algebra course in university has seen the &lt;a href=&#34;https://en.wikipedia.org/wiki/LU_decomposition&#34;&gt;LU decomposition&lt;/a&gt; for square matrices, where you factor&lt;/p&gt;
&lt;p&gt;$$ A = LU $$&lt;/p&gt;
&lt;p&gt;Here, $L$ is a lower-triangular matrix and $U$ is an upper triangular matrix (hence the naming - lower/upper). This factorization is unique with some modest assumptions. If $A$ is invertible, then there is a unique unit-diagonal (which is a fancy way of saying the diagonal is all 1&amp;rsquo;s) $L$ and corresponding unique $U$.&lt;/p&gt;
&lt;p&gt;This factorization is the basis of &lt;a href=&#34;https://en.wikipedia.org/wiki/LINPACK_benchmarks#HPLinpack&#34;&gt;the LINPACK benchmark&lt;/a&gt;, which is used to rank the peak performance of &lt;a href=&#34;https://top500.org/&#34;&gt;the Top 500 supercomputers&lt;/a&gt;. Specifically, the goal is to perform an LU decomposition with &amp;ldquo;partial pivoting&amp;rdquo; (row permutations only). It looks like&lt;/p&gt;
&lt;p&gt;$$ A = PLU $$&lt;/p&gt;
&lt;p&gt;where $P$ is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Permutation_matrix&#34;&gt;permutation matrix&lt;/a&gt;. The $PLU$ variant is more versatile, as there are many more matrices you can factor as $PLU$ than $LU$. But everyone just calls this LU, and the subtlety isn&amp;rsquo;t worth spending much time thinking about.&lt;/p&gt;
&lt;p&gt;LU is useful for things like solving square systems of equations and computing matrix inverses. This is why R uses &lt;code&gt;solve(x, y)&lt;/code&gt; for solving a system of equations and &lt;code&gt;solve(x)&lt;/code&gt; for inverting a matrix. It&amp;rsquo;s not a good interface choice, but there is a kind of logic to it. LU can also be used to calculate the determinant, which R&amp;rsquo;s &lt;code&gt;det()&lt;/code&gt; and &lt;code&gt;determinant()&lt;/code&gt; functions also use.&lt;/p&gt;
&lt;p&gt;Solving a square system of equations is a pretty rare task in data analysis, but inverting things is common enough. Often times an analyst needs the elements of the inverse of a covariance matrix, for example. Although in this case, the matrix is probably symmetric (i.e. the upper and lower triangles are the same) depending on what you mean by &amp;ldquo;a covariance matrix&amp;rdquo;. So you would actually be better off using a Cholesky factorization instead (more on that next section).&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s also worth noting that you usually don&amp;rsquo;t actually need to compute the matrix inverse, but the &lt;em&gt;action&lt;/em&gt; of the inverse. As noted above, statisticians sometimes legitimately need the elements of the inverse of a correlation matrix or whatever. But more often you need to compute something like $A^{-1} B$ rather than just $A^{-1}$ by itself. Yes, there is a difference. In this case, you would be better off doing something like factoring $A=LU$, then using &lt;a href=&#34;https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution&#34;&gt;forward substitution followed by back substitution&lt;/a&gt; with $L$ and $U$, respectively, against $B$. Numerically, this is not the same as actually constructing $A^{-1}$ and then multiplying against $B$. The output may be more accurate if $A$ is ill-conditioned, and it will probably be faster.&lt;/p&gt;
&lt;p&gt;You may be wondering why we&amp;rsquo;ve talked so much about matrix inversion and haven&amp;rsquo;t mentioned something like Gauss-Jordan elimination. That&amp;rsquo;s because this is not actually useful numerically for matrix inversion. Your math professor wasted your time by making you compute a bunch of those problems by hand.&lt;/p&gt;
&lt;p&gt;Your favorite matrix software probably uses the &lt;a href=&#34;https://performance.netlib.org/lapack/&#34;&gt;LAPACK&lt;/a&gt; API for matrix factorizations. There are lots of implementations of the LAPACK interface, including free ones like &lt;a href=&#34;https://www.openblas.net/&#34;&gt;OpenBLAS&lt;/a&gt; and proprietary ones like &lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/math-kernel-library.html&#34;&gt;Intel&amp;rsquo;s MKL&lt;/a&gt;. There are also quasi-clones like &lt;a href=&#34;https://docs.nvidia.com/cuda/cusolver/index.html&#34;&gt;NVIDIA&amp;rsquo;s cuSOLVER&lt;/a&gt;, which has a subset of LAPACK functionality for GPUs. There is also a distributed version called &lt;a href=&#34;https://netlib.org/scalapack/&#34;&gt;ScaLAPACK&lt;/a&gt;. Then there are modern replacements, like &lt;a href=&#34;https://icl.cs.utk.edu/magma/&#34;&gt;MAGMA&lt;/a&gt; and &lt;a href=&#34;https://icl.utk.edu/slate/&#34;&gt;SLATE&lt;/a&gt;. Currently, &lt;a href=&#34;https://github.com/fml-fam/fml&#34;&gt;fml&lt;/a&gt; supports all of these except for MAGMA, which is on the to-do list.&lt;/p&gt;
&lt;p&gt;All of these interfaces follow LAPACK conventions. If your matrix software doesn&amp;rsquo;t, it&amp;rsquo;s because they&amp;rsquo;re hiding those conventions from you &amp;mdash; they&amp;rsquo;re still using LAPACK or something that looks a lot like it. So throughout, I&amp;rsquo;m going to reference the LAPACK function (without leading type)&lt;/p&gt;
&lt;p&gt;Anyway, to factor $A=LU$ in LAPACK, you would use the &lt;code&gt;getrf()&lt;/code&gt; function. This uses the convention that $L$ is unit-diagonal, which is actually important, but more on that in a bit. Once you&amp;rsquo;ve factored your matrix, you can call &lt;code&gt;getrs()&lt;/code&gt; to solve a system of equations, or &lt;code&gt;getri()&lt;/code&gt; to invert the matrix. Or, once factored you could make two calls to &lt;code&gt;trsm()&lt;/code&gt; to use the action of the inverse.&lt;/p&gt;
&lt;p&gt;So why does it matter that the $L$ is unit-diagonal in LAPACK? A common trick in LAPACK and its derivatives is to store factorizations compactly. If you factor $A = PLU$ explicitly, then you need to triple the storage of the input. In practice, the &lt;code&gt;getrf()&lt;/code&gt; routines will replace $A$ by a unit-diagonal $L$ in the lower triangle of the input (not actually storing the diagonal because you know what it is), and the corresponding $U$ in the upper triangle plus diagonal part of the input, with the $P$ information living in a vector of swaps. Why store a bunch of zeros if you don&amp;rsquo;t have to?&lt;/p&gt;
&lt;p&gt;Pretty much all of the LAPACK matrix factorizations will do things like this in one way or another. At the end of the day, your input matrix will be modified, either to store the return or as a kind of temporary workspace when the original data is no longer needed. Your favorite LAPACK bindings (Matlab, R, Armadillo, Julia, numpy, &amp;hellip;) will hide this from you by operating on a copy of the input. This is why operating at the level of LAPACK directly instead of a high-level interface can have massive performance and memory advantages, particularly for iterative algorithms. This is a big motivating reason why we try to stay closer to LAPACK style conventions in &lt;a href=&#34;https://github.com/fml-fam/fml&#34;&gt;fml&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;cholesky&#34;&gt;Cholesky&lt;/h2&gt;
&lt;p&gt;The Cholesky decomposition is a powerful but often overlooked matrix factorization. But first, and most importantly, how do you even say it? My understanding is that this is properly pronounced &amp;ldquo;ko-less-kee&amp;rdquo;, although I think most Anglophones substitute &amp;ldquo;cho&amp;rdquo; for &amp;ldquo;ko&amp;rdquo;. You&amp;rsquo;d have to ask a native French speaker if you wanted to know for sure.&lt;/p&gt;
&lt;p&gt;That out of the way, we can think of the Cholesky decomposition as a factorization of $A = LU$ where $L$ and $U$ are lower/upper triangular, and $L^T = U$. Remember that we could factor many square matrices as $A = PLU$. However, here we additionally require that the matrix be symmetric (its transpose is itself again) and positive definite (eigenvalues are all positive). More on these assumptions in a bit.&lt;/p&gt;
&lt;p&gt;But why even bother with this when we already have the more general LU? For the cases where Cholesky applies, in software it is generally faster to deal with. One of the reasons it&amp;rsquo;s faster is that you know for sure it does not have to pivot, while the LU might (and that&amp;rsquo;s not cheap). Memory consumption should basically be the same, although slightly less for Cholesky since there is no vector of pivots to store.&lt;/p&gt;
&lt;p&gt;Usually the matrices in the Cholesky factorization are referred to as the left and right Cholesky factors. So they are usually denoted as $L$ and $R$. Because of this and the transpose connection between the two factors, you can basically take your pick of:&lt;/p&gt;
&lt;p&gt;$$
A = LR = LL^T = R^TR
$$&lt;/p&gt;
&lt;p&gt;Because the factorization takes a matrix and re-writes it as a product of essentially two copies of the same matrix (one merely transposed), this factorization is often thought of as computing the square root of a matrix. A square root of a number is another number such that when you square it you get the original number back. Similar idea here, but for matrices.&lt;/p&gt;
&lt;p&gt;There are weirdos who deal with actual matrix square roots, where you factor $A = BB$ (no transpose). But I&amp;rsquo;m not aware of any real use for this kind of thing. And there are other &amp;ldquo;functions on matrices&amp;rdquo; that you can do, like the &lt;a href=&#34;https://en.wikipedia.org/wiki/Matrix_exponential&#34;&gt;matrix exponential&lt;/a&gt;. While lots of fun to talk about, this isn&amp;rsquo;t really related to matrix factorizations.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s return for a moment to the assumptions, namely that the matrix is symmetric positive definite. It&amp;rsquo;s generally pretty easy to tell if a matrix is symmetric; positive definite is a bit harder, especially when you&amp;rsquo;re probably only interested in Cholesky for its low compute cost (and eigenvalues are expensive). However, for the applications that statisticians care about, you almost always get both for free.&lt;/p&gt;
&lt;p&gt;If you want to calculate the left/right Cholesky factor of a covariance matrix, then you&amp;rsquo;re in luck! It must be symmetric because if $\Sigma = A^TA$ (I know there&amp;rsquo;s more to computing a covariance matrix but just go with it), then $\sigma_{ij} = A_i \cdot A_j$ for any $i, j$, where $A_i$ is the $i$&#39;th column vector of $A$. And of course the dot product is commutative. To see that it is positive definite intuitively, we&amp;rsquo;re squaring the matrix $A$ to compute $\Sigma$. So we&amp;rsquo;re squaring the eigenvalues, so we can take square roots, so they must be non-negative. Now that&amp;rsquo;s not a proof, but it&amp;rsquo;s correct intuition. We also might get into trouble if one is zero (positive semi-definite), but that&amp;rsquo;s not worth worrying about right now.&lt;/p&gt;
&lt;p&gt;As noted several times, Cholesky is pretty quick, assuming your matrix meets the requirements. You can compute a Cholesky factor with LAPACK&amp;rsquo;s &lt;code&gt;potrf()&lt;/code&gt; function. This will give you either the left or right factor depending on which triangle of the input you told it to use. By convention, R&amp;rsquo;s &lt;code&gt;chol()&lt;/code&gt; will always return the right factor; in &lt;a href=&#34;https://github.com/fml-fam/fml&#34;&gt;fml&lt;/a&gt; we use the left. Once factored, you can use &lt;code&gt;potri()&lt;/code&gt; to compute the inverse, or &lt;code&gt;potrs()&lt;/code&gt; to solve a system of equations.&lt;/p&gt;
&lt;h2 id=&#34;qr&#34;&gt;QR&lt;/h2&gt;
&lt;p&gt;A very useful, but perhaps under-appreciated factorization is the &lt;a href=&#34;https://en.wikipedia.org/wiki/QR_decomposition&#34;&gt;QR factorization&lt;/a&gt;. Let&amp;rsquo;s assume for the moment that $A$ is $m\times n$ with $m \geq n$. Then we can factor&lt;/p&gt;
&lt;p&gt;$$
A_{m\times n} = Q_{m\times n} R_{n\times n}
$$&lt;/p&gt;
&lt;p&gt;where $R$ is upper triangular, and $Q$ is &lt;a href=&#34;https://en.wikipedia.org/wiki/Orthogonal_matrix&#34;&gt;orthogonal&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To say that a matrix is orthogonal is the same as saying that its transpose is its inverse. For a rectangular matrix, this only works along the short dimension. So if $Q$ is orthogonal of dimension $m\times n$ with $m\geq n$ then $Q^TQ = I_{n \times n}$ but $QQ^T$ will not be $I_{m\times m}$ (because the rows of $Q$ can not be linearly independent in this case). But if $m\leq n$ then $Q^TQ = QQ^T = I_{n\times n}$.&lt;/p&gt;
&lt;p&gt;If $m &amp;lt; n$ then you still could factor $A=QR$, but the dimensions work out a little differently. And frankly, I don&amp;rsquo;t think there&amp;rsquo;s ever a good reason to do that. Instead you would use the LQ factorization&lt;/p&gt;
&lt;p&gt;$$
A_{m\times n} = L_{m\times m} Q_{m\times n}
$$&lt;/p&gt;
&lt;p&gt;where $L$ is lower triangular and $Q$ is again orthogonal. This time, $QQ^T = I_{m\times m}$.&lt;/p&gt;
&lt;p&gt;Mathematically, LQ is just QR on a transposed matrix, because $\left(QR\right)^T = R^TQ^T$. But on a computer, you don&amp;rsquo;t want to have to store the matrix transpose if $A$ is large. Most people who say &amp;ldquo;QR&amp;rdquo; really mean &amp;ldquo;QR or LQ, whichever is cheaper&amp;rdquo;, without bothering to explicitly state it. Sadly, this means that poor old LQ doesn&amp;rsquo;t always get the attention it deserves. In fact, R considers LQ so uninteresting that they don&amp;rsquo;t even have LQ analogues of its QR functions. This annoys me more than you might expect. I mean come on, it&amp;rsquo;s the 21st century. They can put a man on the moon, but you can&amp;rsquo;t compute an LQ without a transpose? Really now.&lt;/p&gt;
&lt;p&gt;So right away, this is in a different league than the previous two factorizations. First, we can apply it to rectangular matrices &amp;mdash; you know, the important ones? And while in principle you could use it to solve a square system of equations or even invert a matrix, there&amp;rsquo;s not really a good reason to do that. You already have LU and Cholesky, and QR will be slower for those applications.&lt;/p&gt;
&lt;p&gt;So what can you use it for? One very useful application of QR is linear regression. You can build a linear model fitter fairly directly using QR. For generalized linear models, you can use &lt;a href=&#34;https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares&#34;&gt;iteratively reweighted least squares&lt;/a&gt; with the linear model fitter as the core driver.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve gotten into borderline fist fights with statisticians when I say that &amp;ldquo;linear regression &lt;em&gt;is&lt;/em&gt; linear least squares&amp;rdquo;. Disagreement usually lives in whether or not you mean &amp;ldquo;ANOVA&amp;rdquo; when you say &amp;ldquo;linear regression&amp;rdquo;. Anyway I&amp;rsquo;m right and they&amp;rsquo;re wrong, and in linear least square you have a data matrix $X$ and response vector $y$ and you want to find a parameter vector $\beta$ that minimizes&lt;/p&gt;
&lt;p&gt;$$
\lVert X_{m\times n}\beta_{n\times 1} - y_{m\times 1} \lVert
$$&lt;/p&gt;
&lt;p&gt;If $X$ is full rank, then this has analytical solution&lt;/p&gt;
&lt;p&gt;$$
\beta = \left( X^T X \right)^{-1} X^T y
$$&lt;/p&gt;
&lt;p&gt;Often $X$ is not full rank, and in that case your cute little stats101 trick won&amp;rsquo;t work. It&amp;rsquo;s also a bad way to compute this in floating point arithmetic regardless of the column rank of the input; but to really get into that I need to talk about condition numbers, and I will do that in a later post. For now it&amp;rsquo;s worth pointing out that any statistical software written by anyone who remotely knows what they&amp;rsquo;re doing won&amp;rsquo;t implement linear regression this way.&lt;/p&gt;
&lt;p&gt;So what &lt;em&gt;is&lt;/em&gt; the correct way to do it? Note that $\lVert Q \lVert = \lVert Q^T \lVert = 1$ and $\lVert AB \lVert \leq \lVert A \lVert \lVert B \lVert$, so&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\lVert X\beta - y \lVert &amp;amp;= \lVert QR\beta - y \lVert \\&lt;br&gt;
&amp;amp;= \lVert Q^TQR\beta - Q^T y \lVert \\&lt;br&gt;
&amp;amp;= \lVert R\beta - Q^T y \lVert
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;This is equivalent to solving the system&lt;/p&gt;
&lt;p&gt;$$
R\beta = Q^T y
$$&lt;/p&gt;
&lt;p&gt;Because $R$ is triangular, this can be done quickly and accurately using triangular solvers, like LAPACK&amp;rsquo;s &lt;code&gt;trtrs()&lt;/code&gt; functions. If your system is under-determined (i.e. $m&amp;lt;n$), you can play a similar game with LQ, but the math is a little more annoying.&lt;/p&gt;
&lt;p&gt;Interestingly, R uses a modified version of the QR routine in the ancient predecessor to LAPACK called &lt;a href=&#34;https://www.netlib.org/linpack/&#34;&gt;LINPACK&lt;/a&gt; &amp;mdash; no, not the benchmark. They use this because QR solvers will make choices that can destroy information useful in ANOVA if you started with a rank degenerate model matrix.&lt;/p&gt;
&lt;p&gt;For the sake of numerical stability, the LAPACK QR driver will choose the column with largest partial norm for pivoting while forming the Householder reflections. However, this can re-order the columns so that a statistical interpretation
in an ANOVA is destroyed. The modifications in R&amp;rsquo;s QR driver (used in &lt;code&gt;lm()&lt;/code&gt; and &lt;code&gt;lm.fit()&lt;/code&gt;) implements a &amp;ldquo;limited column pivoting&amp;rdquo; strategy. Here, when a column is detected to have &amp;ldquo;small&amp;rdquo; partial norm, it is pushed to the back of the columns. The author of the modifications (who I believe to be Ross Ihaka) wrote this in the comments:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a limited column pivoting strategy based on the 2-norms of the reduced columns moves columns with near-zero norm to the right-hand edge of the x matrix. this strategy means that sequential one degree-of-freedom effects can be computed in a natural way.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;i am very nervous about modifying linpack code in this way. if you are a computational linear algebra guru and you really understand how to solve this problem please feel free to suggest improvements to this code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For what it&amp;rsquo;s worth, I&amp;rsquo;ve spoken to several members of the LAPACK team and they seem to believe that this strategy is essentially fine numerically. Outside of R&amp;rsquo;s linear model fitter, I modified the ScaLAPACK linear model fitter to support the limited pivoting strategy in &lt;a href=&#34;https://github.com/RBigData/pbdBASE&#34;&gt;the pbdBASE package&lt;/a&gt;. I suspect basically every quality statistical software does something like this, although I do not know for sure.&lt;/p&gt;
&lt;p&gt;LAPACK has a least squares driver &lt;code&gt;gels()&lt;/code&gt; that will automatically use QR or LQ to find the least squares solution. If you want to do a QR directly, there are a few options; some pivot, some don&amp;rsquo;t. Pivoting is necessary if your matrix is (or might be if you don&amp;rsquo;t know) rank degenerate. If it does no pivoting, then it is basically assuming that the matrix has full column rank. The pivoting ones can be used as so-called &amp;ldquo;rank-revealing&amp;rdquo; QR. If pivoting, you would probably want &lt;code&gt;geqp3()&lt;/code&gt;, and if not you would use &lt;code&gt;geqrf()&lt;/code&gt;. Replace &lt;code&gt;qr&lt;/code&gt; with &lt;code&gt;lq&lt;/code&gt; for the LQ variants.&lt;/p&gt;
&lt;p&gt;Like with LU, the QR factorization from LAPACK is stored compactly, replacing the input matrix. However, the way it is stored is quite a bit more complicated than LU. For QR, the upper triangle plus diagonal will be the $R$ matrix. Obviously the lower triangle can&amp;rsquo;t be $Q$, because it&amp;rsquo;s not a lower triangular matrix. Instead, you would use the additional function &lt;code&gt;ormqr()&lt;/code&gt; to operate on the action of $Q$ or its transpose. If you really want the elements of $Q$, you can use &lt;code&gt;orgqr()&lt;/code&gt; which is in-place, or use &lt;code&gt;ormqr()&lt;/code&gt; against a diagonal matrix of all 1&amp;rsquo;s (a &amp;ldquo;rectangular identity matrix&amp;rdquo;). The same things all hold true for LQ.&lt;/p&gt;
&lt;h2 id=&#34;singular-value-decomposition&#34;&gt;Singular Value Decomposition&lt;/h2&gt;
&lt;p&gt;Probably the most important and powerful of the factorizations is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34;&gt;singular value decomposition&lt;/a&gt;, aka SVD.&lt;/p&gt;
&lt;p&gt;If $A$ is an $m\times n$ matrix then&lt;/p&gt;
&lt;p&gt;$$
A_{m\times n} = U_{m\times k} \Sigma_{k\times k} V_{n\times k}^T
$$&lt;/p&gt;
&lt;p&gt;where $k$ is usually the minimum of $m$ and $n$ (although it may be taken to be the rank of $A$ which is no greater than the minimum of $m$ and $n$), $\Sigma$ is a diagonal matrix, and $U$ and $V$ are orthogonal. This is sometimes called the &amp;ldquo;compact SVD&amp;rdquo;. Although it&amp;rsquo;s basically never done in software, we could take the &amp;ldquo;full SVD&amp;rdquo; as&lt;/p&gt;
&lt;p&gt;$$
A_{m\times n} = U_{m\times m} \Sigma_{m\times n} V^T_{n\times n}
$$&lt;/p&gt;
&lt;p&gt;When I said SVD is the most powerful factorization, I wasn&amp;rsquo;t joking. You can do &lt;em&gt;&lt;strong&gt;anything&lt;/strong&gt;&lt;/em&gt; with an SVD. Matrix inverse? Easy&lt;/p&gt;
&lt;p&gt;$$
A^{-1} = V \Sigma^{-1} U^T
$$&lt;/p&gt;
&lt;p&gt;Solve a system of equations? Don&amp;rsquo;t make me laugh&lt;/p&gt;
&lt;p&gt;$$
Ax=b \iff x = V \Sigma^{-1} U^Tb
$$&lt;/p&gt;
&lt;p&gt;Determinant? Are you even trying?&lt;/p&gt;
&lt;p&gt;$$
det(A) = det(U \Sigma V^T) = det(\Sigma) = \displaystyle\prod_{i=1}^n \sigma_{ii}
$$&lt;/p&gt;
&lt;p&gt;Linear regression? HAVE YOU EVEN BEEN PAYING ATTENTION?&lt;/p&gt;
&lt;p&gt;$$
y = X\beta \iff \beta = V \Sigma^{-1} U^T y
$$&lt;/p&gt;
&lt;p&gt;Condition number? SVD.&lt;/p&gt;
&lt;p&gt;Column rank? SVD.&lt;/p&gt;
&lt;p&gt;PCA &lt;em&gt;&lt;strong&gt;is&lt;/strong&gt;&lt;/em&gt; SVD.&lt;/p&gt;
&lt;p&gt;SVD is &lt;em&gt;love&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;SVD is &lt;em&gt;life&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;ALL HAIL SVD&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Frankly I&amp;rsquo;m not convinced there&amp;rsquo;s anything you can do with a matrix that can&amp;rsquo;t be improved with an SVD. Almost all matrix math problems get easier when you take the SVD. Mathematically, it is strictly better than QR. Computationally, it is more expensive to compute and needs more memory.&lt;/p&gt;
&lt;p&gt;SVD being so obviously important, a lot of study and care has been devoted to calculating this factorization. The core technique for most solvers uses something called the &lt;a href=&#34;https://en.wikipedia.org/wiki/QR_algorithm&#34;&gt;QR algorithm&lt;/a&gt;. You may be surprised to learn that this involves a QR factorization, although it&amp;rsquo;s a bit more involved than I want to get into.&lt;/p&gt;
&lt;p&gt;LAPACK has several routines for computing singular values/vectors. Probably the most commonly used are &lt;code&gt;gesvd()&lt;/code&gt; and &lt;code&gt;gesdd()&lt;/code&gt;. The first uses the QR algorithm, while the second uses a divide-and-conquer algorithm. If you are only computing singular values, they should be the same. For computing singular vectors, the divide-and-conquer approach should be much faster. LAPACK also includes &lt;code&gt;gesvdj()&lt;/code&gt;, which uses Jacobi rotations to compute the SVD. Personally I never see any reason to use these over &lt;code&gt;gesdd()&lt;/code&gt;, but I encourage you to explore for yourself.&lt;/p&gt;
&lt;p&gt;All of these functions will compute all singular values and/or vectors (for the compact SVD), even if you just need a few. In R&amp;rsquo;s &lt;code&gt;La.svd()&lt;/code&gt; and &lt;code&gt;svd()&lt;/code&gt; bindings, you can request a subset of singular vectors. But actually, your choices are none of them or all of them (you can compute only the singular values without any singular vectors though). So if you only want one, it will compute all of them and then throw away what you didn&amp;rsquo;t want.&lt;/p&gt;
&lt;p&gt;If you only want a few singular vectors (e.g. making a plot from the first few principal components), you may be interested in a &amp;ldquo;truncated SVD&amp;rdquo;, not to be confused with &amp;ldquo;compact SVD&amp;rdquo;. There are some common techniques for approximating truncated SVD. There is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Lanczos_algorithm&#34;&gt;Lanczos algorithm&lt;/a&gt;, which I wrote about extensively &lt;a href=&#34;https://fml-fam.github.io/blog/blog/2020/06/15/svd-via-lanczos-iteration/&#34;&gt;in a previous post&lt;/a&gt;. Then there&amp;rsquo;s my personal favorite method, that uses &lt;a href=&#34;https://arxiv.org/abs/0909.4061&#34;&gt;random projections&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Sometimes, especially in algorithmic explanations, SVD will only be described for square matrices. There&amp;rsquo;s a fun little trick you can do to reduce a rectangular matrix to a square one for computing SVD. Let&amp;rsquo;s say $A_{m\times n}$ is a rectangular matrix with $m&amp;gt;n$, and you know how to compute the SVD of a square matrix. Then we can factor $A=QR$ first, and take the SVD of the square matrix $R$. It goes something like this:&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
A &amp;amp;= QR \\&lt;br&gt;
&amp;amp;= Q \left( U_R \Sigma_R V_R^T \right) \\&lt;br&gt;
&amp;amp;= \left( Q U_R \right) \Sigma_R V_R^T
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Then $U_A = Q U_R$ because a product of orthogonal matrices is orthogonal, and $\Sigma_A = \Sigma_R$ and $V_A = V_R$. The LAPACK routines discussed above use this strategy.&lt;/p&gt;
&lt;h2 id=&#34;eigendecomposition&#34;&gt;Eigendecomposition&lt;/h2&gt;
&lt;p&gt;Given a square, symmetric matrix $A$ of order $n$, its eigenvalue decomposition (aka &amp;ldquo;eigendecomposition&amp;rdquo;, aka &amp;ldquo;spectral decomposition&amp;rdquo;) is&lt;/p&gt;
&lt;p&gt;$$
A = V \Lambda V^T
$$&lt;/p&gt;
&lt;p&gt;where $V$ is an orthogonal matrix and $\Lambda$ is diagonal, each of order $n$. Both are guaranteed to be real-valued since $A$ is symmetric. You can discuss eigendecompositions for non-symmetric matrices, but both the eigenvalues (diagonals of the $\Lambda$ matrix) and eigenvectors are no longer guaranteed to be real-valued, even if $A$ is. The reason why is explored in the fundamental theorem of algebra.&lt;/p&gt;
&lt;p&gt;In school, everyone learns about eigenvalues because they&amp;rsquo;re important to engineers. For data analysis, they are largely unimportant. They can be used to compute the &lt;a href=&#34;https://en.wikipedia.org/wiki/Trace_%28linear_algebra%29&#34;&gt;trace&lt;/a&gt; (i.e., the sum of the diagonal) of a square matrix in the most computationally expensive way imaginable. They can also be used to compute the determinant, although it is generally much faster to use LU.&lt;/p&gt;
&lt;p&gt;Probably the only reason anyone interested in data analysis would actually care about spectral decomposition is because of its interesting relationship to SVD. If $A$ is a rectangular matrix and $A = U\Sigma V^T$ then&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
A^TA &amp;amp;= \left( U \Sigma V^T \right)^T \left( U \Sigma V^T \right) \\&lt;br&gt;
&amp;amp;= V \Sigma U^T U \Sigma V^T \\&lt;br&gt;
&amp;amp;= V \Sigma^2 V^T
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;So the eigenvalues of $A^TA$ are the square of the singular values of $A$, and the eigenvectors of $A^TA$ are the right singular vectors of $A$. If we know $\Sigma$ and $V$ then we can recover $U$ easily since $U = A V \Sigma^{-1}$.&lt;/p&gt;
&lt;p&gt;Likewise, $AA^T = U \Sigma^2 U^T$.&lt;/p&gt;
&lt;p&gt;This trick is how many big data frameworks will compute SVD. If $n$ is small in absolute terms, then you can distribute you matrix across multiple processes by rows. Once you compute $A^TA$ &amp;mdash; which will require communication &amp;mdash; all the remaining operations are local. For the product, you first compute the local product, then add up all of the local matrices. This is equivalent to an MPI allreduce operation. After that, the eigendecomposition is local, and you can use LAPACK or whatever.&lt;/p&gt;
&lt;p&gt;The advantages to this strategy are that it can be very quick computationally, and it&amp;rsquo;s very easy to implement. But there are some issues with this approach. For one, expressly computing $A^TA$ or $AA^T$ will square the &lt;a href=&#34;https://en.wikipedia.org/wiki/Condition_number&#34;&gt;condition number&lt;/a&gt; of $A$. I will have more to say about condition numbers in a later post, but let&amp;rsquo;s just say for now that this is a bad thing. So if you are doing your computing in floating point and accuracy is important, this may not be a good strategy for computing singular values. It would be better to use something like a tall skinny QR algorithm (TSQR). I don&amp;rsquo;t feel like getting into what that is right now, but trust me, it&amp;rsquo;s a real thing.&lt;/p&gt;
&lt;p&gt;The other major issue staring us in the face is, of course, how we handle the case where $n$ is large in absolute terms, or if $m$ and $n$ are &amp;ldquo;kinda large&amp;rdquo; but $A$ is nearly square. Then computing either $A^TA$ or $AA^T$ potentially consumes a lot of memory.&lt;/p&gt;
&lt;p&gt;In LAPACK, you have quite a few choices for symmetric eigensolvers. The two I think are most immediately worth looking at are the relatively robust representations method implemented in &lt;code&gt;syevr()&lt;/code&gt; and the divide-and-conquer method implemented in &lt;code&gt;syevd()&lt;/code&gt;. Years ago, &lt;a href=&#34;http://librestats.com/2016/10/28/comparing-symmetric-eigenvalue-performance/&#34;&gt;I wrote about the differences&lt;/a&gt; between these two in yet another way too lengthy post. The tldr is that the divide-and-conquer method is more scalable for multicore but uses a TON of memory.&lt;/p&gt;
&lt;p&gt;There are other libraries like &lt;a href=&#34;https://www.netlib.org/eispack/&#34;&gt;EISPACK&lt;/a&gt; (har har har, get it?) and &lt;a href=&#34;https://www.caam.rice.edu/software/ARPACK/&#34;&gt;ARPACK&lt;/a&gt;. But really all you need is in LAPACK, its clones like cuSOLVER, or its modern variants like &lt;a href=&#34;https://icl.cs.utk.edu/magma/&#34;&gt;MAGMA&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Like with SVD, I&amp;rsquo;m sure there are lots of ways of computing truncated eigendecompositions, although I don&amp;rsquo;t know anything about them because again, it&amp;rsquo;s a mostly useless factorization for data analysis. If you only want one eigen value/vector, you can use &lt;a href=&#34;https://en.wikipedia.org/wiki/Power_iteration&#34;&gt;power iteration&lt;/a&gt;, which is actually related to the matrix exponential; but I&amp;rsquo;m not getting into that now.&lt;/p&gt;
&lt;h2 id=&#34;others&#34;&gt;Others&lt;/h2&gt;
&lt;p&gt;In my opinion, these make up the &amp;ldquo;core&amp;rdquo; matrix factorizations, particularly for data scientists. There are of course others, although many are what I would call less useful generalizations of the above (e.g. &lt;a href=&#34;https://en.wikipedia.org/wiki/LU_decomposition#LDU_decomposition&#34;&gt;LDU&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Schur_decomposition#Generalized_Schur_decomposition&#34;&gt;QZ&lt;/a&gt;). There are also some more specialized things like &lt;a href=&#34;https://en.wikipedia.org/wiki/Non-negative_matrix_factorization&#34;&gt;non-negative matrix factorization&lt;/a&gt;. Also, apparently k-means clustering &lt;a href=&#34;https://arxiv.org/abs/1512.07548&#34;&gt;can be formulated as a matrix factorization problem&lt;/a&gt;, although I think this is more interesting than it is useful.&lt;/p&gt;
&lt;p&gt;If you studied math in school, you may well have spent ages talking about &lt;a href=&#34;https://en.wikipedia.org/wiki/Jordan_normal_form&#34;&gt;the Jordan canonical form&lt;/a&gt; without ever hearing about anything actually useful like SVD (I did). These kinds of formulations are interesting to mathematicians because they are powerful at proving yet more things that are equally useless. Best to ignore them.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;p&gt;Well this kind of got out of hand. I didn&amp;rsquo;t mean to turn this into a book. But here we are, so I figure the least I can do is give you some problems.&lt;/p&gt;
&lt;p&gt;Math problems&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let $A$ be a square matrix of order $n$ and let $A = PLU$.
&lt;ol&gt;
&lt;li&gt;What is the contribution of $P$ to the determinant of $A$? Recall that $det(AB) = det(A)det(B)$.&lt;/li&gt;
&lt;li&gt;Describe a strategy to calculate its determinant using this factorization. You may assume without proof that $det(T) = \displaystyle\prod_{i=1}^n t_{ii}$ for a triangular matrix $T_{n\times n}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Let $A$ be a square matrix of order $n$.
&lt;ol&gt;
&lt;li&gt;If $A$ is positive definite, describe a strategy to calculate its determinant using its cholesky factorization.&lt;/li&gt;
&lt;li&gt;What if $A$ is positive semi-definite but not positive definite? (hint: see problem 7 part 1)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Prove that a product of 2 orthogonal matrices is again orthogonal.&lt;/li&gt;
&lt;li&gt;Prove that if $Q$ is a square orthogonal matrix, its determinant must be 1 or -1 (hint: start with $Q^TQ = I$).&lt;/li&gt;
&lt;li&gt;If a square matrix has determinant 1, is it necessarily orthogonal?&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Let $X$ be an $m\times n$ matrix with $m &amp;lt; n$ and let $y$ be an $m$-length vector. Use $X=LQ$ to solve the linear least square problem $\displaystyle\min_\beta \lVert X\beta - y \lVert$. Are there any subtleties here for implementing this in software?&lt;/li&gt;
&lt;li&gt;Let $A$ be a square, symmetric matrix of order $n$, and let $A = U \Sigma V^T$ be its SVD.
&lt;ol&gt;
&lt;li&gt;Show that $U = V$.&lt;/li&gt;
&lt;li&gt;Assume that each singular value $\sigma_i \geq 0$. Given this, find the matrix square root of $A$, that is, the matrix $B$ so that $BB = A$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Let $A$ be a square matrix of order $n$. The polar decomposition is a matrix factorization where $A = UP$, where $U$ is orthogonal and $P$ is positive semi-definite, each of order $n$. Derive $U$ and $P$ via the SVD (hint: $U_\text{polar}$ is not $U_\text{svd}$).&lt;/li&gt;
&lt;li&gt;Let $A$ be a square symmetric matrix of order $n$.
&lt;ol&gt;
&lt;li&gt;Use its eigendecomposition to calculate $det(A)$.&lt;/li&gt;
&lt;li&gt;Using only its eigendecomposition, show that the trace of $A$ is equal to the sum of the eigenvalues (hint: first show that by combining the definitions of trace and matrix product, $tr(AB) = tr(BA)$; or use your fancy inner product math if you know that).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Programming problems&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Implement a function that can solve a (square) system of equations $Ax=b$ by factoring $A=LU$, but without ever constructing $A^{-1}$. You may use your programming language&amp;rsquo;s triangular system solvers (e.g., &lt;code&gt;backsolve()&lt;/code&gt; and &lt;code&gt;forwardsolve()&lt;/code&gt; in R).&lt;/li&gt;
&lt;li&gt;Construct a symmetric matrix $A$ of order $n$ of random normal values.
&lt;ol&gt;
&lt;li&gt;Compare the run time computing the inverse using a general matrix inverter (LU) vs using Cholesky. Try with various values of $n$.&lt;/li&gt;
&lt;li&gt;Compare the run time computing the determinant with an implementation that uses LU (e.g. R&amp;rsquo;s &lt;code&gt;det()&lt;/code&gt;) to one using Cholesky. Try with various values of $n$.&lt;/li&gt;
&lt;li&gt;When was Cholesky better than LU? What are the tradeoffs to each approach?&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Let $X$ be an $m\times n$ matrix with $m&amp;gt;n$ and $y$ an $m$-length vector.
&lt;ol&gt;
&lt;li&gt;Implement a function &lt;code&gt;lm_xtx()&lt;/code&gt; that takes arguments $X$ and $y$ and computes the linear regression coefficients $\beta$ using the analytical solution.&lt;/li&gt;
&lt;li&gt;Implement a function &lt;code&gt;lm_qr()&lt;/code&gt; that takes arguments $X$ and $y$ and computes the linear regression coefficients $\beta$ using the QR formulation.&lt;/li&gt;
&lt;li&gt;Let $X$ be the $7\times 7$ &lt;a href=&#34;https://en.wikipedia.org/wiki/Hilbert_matrix&#34;&gt;Hilbert matrix&lt;/a&gt;, and let $y$ be $[1, 2, 3, 4, 5, 6, 7]^T$. Test your &lt;code&gt;lm_qr()&lt;/code&gt; function first, then test &lt;code&gt;lm_xtx()&lt;/code&gt;. Did anything interesting happen?&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Implement a matrix square root function (use problem 5 above). If the input is an invalid matrix under our assumptions (non-square, non-symmetric, non- positive semi-definite), it should detect this and throw an error.&lt;/li&gt;
&lt;li&gt;For each of the following, assume all matrix inputs are square and symmetric.
&lt;ol&gt;
&lt;li&gt;Implement a function to compute the determinant using eigendecomposition. Compare the run time performance to an implementation that uses LU on a random matrix. Try many different matrix sizes.&lt;/li&gt;
&lt;li&gt;Implement a function to compute the trace using eigendecomposition. Compare the run time performance to simply summing the diagonal of the input. Try many different matrix sizes.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Challenge problems&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(difficulty: ruthless shilling) Use &lt;a href=&#34;https://github.com/fml-fam/fml&#34;&gt;fml&lt;/a&gt; or &lt;a href=&#34;https://github.com/fml-fam/fmlr&#34;&gt;fmlr&lt;/a&gt; to experiment with the programming problems above.&lt;/li&gt;
&lt;li&gt;(difficulty: fortran) Go learn about the strange naming scheme for LAPACK functions.&lt;/li&gt;
&lt;li&gt;(difficulty: &amp;ldquo;we&amp;rsquo;re, like, all the same, man&amp;rdquo;) Compare the LU, Cholesky, QR, SVD, and Eigendecomposition API&amp;rsquo;s across several of the mainstream languages (R, Matlab, Julia, python&amp;rsquo;s numpy, &amp;hellip;).&lt;/li&gt;
&lt;li&gt;(difficulty: impossible) Find something SVD isn&amp;rsquo;t good for.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Vignette-less Articles With pkgdown</title>
      <link>https://fml-fam.github.io/blog/2020/06/23/vignette-less-articles-with-pkgdown/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2020/06/23/vignette-less-articles-with-pkgdown/</guid>
      <description>&lt;h2 id=&#34;long-boring-background-you-can-skip&#34;&gt;Long Boring Background You Can Skip&lt;/h2&gt;
&lt;p&gt;I don&amp;rsquo;t like the R vignette system. At its core, it&amp;rsquo;s a good idea. Having long-form package documentation is a good thing, and having the ability to put in source code listings that get automatically executed during rebuilds is great. But the way it works in practice is, to me, extremely annoying. Whenever you build/test your package, the vignettes will automatically be rebuilt, even if that&amp;rsquo;s not what you want. The underlying assumptions here are that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rebuilding vignettes is cheap&lt;/li&gt;
&lt;li&gt;vignette code can run on the machine that is building/checking the package&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both of these assumptions regularly fail for me, and likely anyone else who writes HPC packages for R. Imagine you&amp;rsquo;re benchmarking something that takes tens of minutes or more. You probably don&amp;rsquo;t want to re-run those very often. You might only even want to run them on a completely different system, say one with more RAM or CPU cores, or even one with a fancy GPU that your dev box doesn&amp;rsquo;t have.&lt;/p&gt;
&lt;p&gt;I also don&amp;rsquo;t like that if you want to have html documents, you need to list a fairly heavy package, &lt;a href=&#34;https://cran.r-project.org/web/packages/knitr/index.html&#34;&gt;knitr&lt;/a&gt;, in your package DESCRIPTION&amp;rsquo;s Suggests field. On Linux (the only platform I run on), this is doubly annoying because everything has to be built from source. You can of course install a package without suggested packages by modifying the &lt;code&gt;dependencies&lt;/code&gt; argument of &lt;code&gt;install.packages()&lt;/code&gt;. But even as annoyed as I get by a giant wall of suggested packages needlessly cluttering my R library, I usually don&amp;rsquo;t bother because by the time I realize the problem, it&amp;rsquo;s already downloading and installing them.&lt;/p&gt;
&lt;p&gt;But html or pdf, if you want to re-build your vignettes infrequently (having qasi-static vignettes), there are a few workarounds I&amp;rsquo;m aware of. One obvious way is to simply not have automatically executed code in your vignettes. I have done this in many packages, and if I&amp;rsquo;m being honest, there is probably some dead/no-longer-working code lying around in those vignettes somewhere. If you restrict yourself to pdf outputs, you can also make a &amp;ldquo;shell&amp;rdquo; vignette that &amp;ldquo;includes&amp;rdquo; your static vignette. But I find in practice that this can be cumbersome, and even then, you&amp;rsquo;re stuck with a pdf.&lt;/p&gt;
&lt;h2 id=&#34;using-pkgdown&#34;&gt;Using pkgdown&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ve recently started using the &lt;a href=&#34;https://pkgdown.r-lib.org/&#34;&gt;pkgdown package&lt;/a&gt; to generate html files for the manpage-style help that R packages have. But it can also handle vignettes in its &amp;ldquo;article&amp;rdquo; system. This gives you the ability to have vignettes without even having to bother with R&amp;rsquo;s vignette system.&lt;/p&gt;
&lt;p&gt;Although the setup is a bit tedious, I find this approach fairly inoffensive, and the results make me very happy. It goes something like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add the line &lt;code&gt;vignettes/&lt;/code&gt; to your package &lt;code&gt;.Rbuildignore&lt;/code&gt; file.&lt;/li&gt;
&lt;li&gt;Set up your vignettes.
&lt;ul&gt;
&lt;li&gt;Put the source Rmd files in, say, &lt;code&gt;vignettes/src/&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Manually build them using &lt;code&gt;rmarkdown::render()&lt;/code&gt; with outputs in &lt;code&gt;vignettes/&lt;/code&gt;, and with output format &lt;code&gt;md_document()&lt;/code&gt; and file extension &lt;code&gt;.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build the pkgdown &amp;ldquo;articles&amp;rdquo; (vignettes).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This gives you complete control over when your documents are rebuilt. I use exactly this workflow in the &lt;a href=&#34;https://github.com/fml-fam/fmlr&#34;&gt;fmlr package&lt;/a&gt;. You can see the generated documentation &lt;a href=&#34;https://fml-fam.github.io/fmlr/html/index.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will have more to say about fmlr in a later post. For now, I&amp;rsquo;ll walk through these pkgdown vignette steps in more depth.&lt;/p&gt;
&lt;h2 id=&#34;1-ignoring-the-vignettes&#34;&gt;1. Ignoring the Vignettes&lt;/h2&gt;
&lt;p&gt;The whole point of this is to not use R&amp;rsquo;s vignette system. So you need to make sure your &lt;code&gt;.Rbuildignore&lt;/code&gt; file contains a line like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vignettes/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;so that when you &lt;code&gt;R CMD build&lt;/code&gt; your package, it won&amp;rsquo;t include any of the vignettes. Your articles still have to go in the &lt;code&gt;vignettes/&lt;/code&gt; folder, because that&amp;rsquo;s where pkgdown will look for them, and I have no idea how to override this if you wanted to put them somewhere else.&lt;/p&gt;
&lt;p&gt;The generated vignette files (output of &lt;code&gt;rmarkdown::render()&lt;/code&gt;) will go in &lt;code&gt;vignettes/&lt;/code&gt;. The raw/source vignettes that contain yet-to-be-executed source code listings should go somewhere else. I use &lt;code&gt;vignettes/src/&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;2-setting-up-your-vignette-builder&#34;&gt;2. Setting Up Your Vignette Builder&lt;/h2&gt;
&lt;p&gt;You need some kind of script that will call &lt;code&gt;rmarkdown::render()&lt;/code&gt; on your source Rmd files. It can be a simple Makefile or a more complicated R script. The format to pass to &lt;code&gt;rmarkdown::render()&lt;/code&gt; should look something like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;fmt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rmarkdown&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;md_document&lt;/span&gt;(
  variant&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gfm&amp;#34;&lt;/span&gt;,
  preserve_yaml&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;,
  ext&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.Rmd&amp;#34;&lt;/span&gt;
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The goal here is to have an Rmd output that has executed the source code listings of your input file. These generated Rmd files are pretending to be the source vignettes for pkgdown.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the builder script that I use:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/usr/bin/env Rscript&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(rmarkdown)

rmf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(f)
{
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;file.exists&lt;/span&gt;(f))
    &lt;span style=&#34;color:#a6e22e&#34;&gt;file.remove&lt;/span&gt;(f)
}

clean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;()
{
  files &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dir&lt;/span&gt;(pattern&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*.Rmd&amp;#34;&lt;/span&gt;, recursive&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FALSE&lt;/span&gt;)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(f in files)
    &lt;span style=&#34;color:#a6e22e&#34;&gt;rmf&lt;/span&gt;(f)
}

set_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;()
{
  &lt;span style=&#34;color:#a6e22e&#34;&gt;while &lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;file.exists&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DESCRIPTION&amp;#34;&lt;/span&gt;))
  {
    &lt;span style=&#34;color:#a6e22e&#34;&gt;setwd&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;..&amp;#34;&lt;/span&gt;)
    &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;getwd&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/home&amp;#34;&lt;/span&gt;)
      &lt;span style=&#34;color:#a6e22e&#34;&gt;stop&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;couldn&amp;#39;t find package!&amp;#34;&lt;/span&gt;)
  }
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;setwd&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;vignettes&amp;#34;&lt;/span&gt;)
}

build_vignette &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(f)
{
  f_Rmd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;basename&lt;/span&gt;(f)
  of &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sub&lt;/span&gt;(f_Rmd, pattern&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;^_&amp;#34;&lt;/span&gt;, replacement&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;rmf&lt;/span&gt;(of)
  
  fmt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rmarkdown&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;md_document&lt;/span&gt;(
    variant&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gfm&amp;#34;&lt;/span&gt;,
    preserve_yaml&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;,
    ext&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.Rmd&amp;#34;&lt;/span&gt;
  )
  
  rmarkdown&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;render&lt;/span&gt;(
    f,
    output_file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;of,
    output_dir&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getwd&lt;/span&gt;(),
    output_format&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;fmt
  )
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;invisible&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;)
}



&lt;span style=&#34;color:#75715e&#34;&gt;# -----------------------------------------------------------------------&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;set_path&lt;/span&gt;()

&lt;span style=&#34;color:#a6e22e&#34;&gt;clean&lt;/span&gt;()
&lt;span style=&#34;color:#a6e22e&#34;&gt;build_vignette&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./src/_source_vignette_1.Rmd&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;build_vignette&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./src/_source_vignette_2.Rmd&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I preface my raw source files with an underscore so that they&amp;rsquo;re immediately visually distinct to me in my editor. If you want to do something else, you may need to slightly modify the logic a bit. Also, because I may not want to rebuild every vignette every time, I just comment out &lt;code&gt;clean()&lt;/code&gt; or &lt;code&gt;build_vignette()&lt;/code&gt; calls as necessary for whatever I&amp;rsquo;m doing. You could easily come up with something more sophisticated, but this works fine for me.&lt;/p&gt;
&lt;h2 id=&#34;3-building-the-pkgdown-articles&#34;&gt;3. Building the pkgdown Articles&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s a sample Makefile for pkgdown. I like to build my documentation in &lt;code&gt;docs/html/&lt;/code&gt;, so I put the Makefile in &lt;code&gt;docs/&lt;/code&gt;. I also like to serve the documentation on GitHub pages (there&amp;rsquo;s an automatic &lt;code&gt;docs/&lt;/code&gt; option in the repo settings), so I have an &lt;code&gt;index.html&lt;/code&gt; re-directer in &lt;code&gt;docs/&lt;/code&gt; as well. Set your override arguments appropriately if you do something different.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;all: docs

docs:
	( Rscript -e &amp;quot;pkgdown::build_site(&#39;..&#39;, override=list(destination=&#39;docs/html&#39;), install=FALSE)&amp;quot; )

articles: vignettes
	( Rscript -e &amp;quot;pkgdown::build_articles(&#39;..&#39;, override=list(destination=&#39;docs/html&#39;))&amp;quot; )

vignettes:
	( cd ../vignettes &amp;amp;&amp;amp; Rscript ./rebuild.r )

clean:
	rm -rf html
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now all you have to do is (re-)build your vignettes with &lt;code&gt;make vignettes&lt;/code&gt;, and &lt;code&gt;make docs&lt;/code&gt; to re-generate the pkgdown html files. If you are making a lot of changes to the vignettes and only want to generate those (skipping the manpage documentation, which can take a while to generate), you can just do &lt;code&gt;make articles&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing fml - the Fused Matrix Library</title>
      <link>https://fml-fam.github.io/blog/2020/06/21/introducing-fml-the-fused-matrix-library/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2020/06/21/introducing-fml-the-fused-matrix-library/</guid>
      <description>&lt;h2 id=&#34;what-is-fml&#34;&gt;What is fml?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/fml-fam/fml&#34;&gt;fml&lt;/a&gt; is the Fused Matrix Library, a &lt;a href=&#34;https://opensource.org/licenses/BSL-1.0&#34;&gt;permissively licensed&lt;/a&gt;, header-only C++ library for dense matrix computing. The emphasis is on real-valued matrix types (&lt;code&gt;float&lt;/code&gt;, &lt;code&gt;double&lt;/code&gt;, and &lt;code&gt;__half&lt;/code&gt;) for numerical operations useful for data analysis.&lt;/p&gt;
&lt;p&gt;The library provides 3 main classes: &lt;code&gt;cpumat&lt;/code&gt;, &lt;code&gt;gpumat&lt;/code&gt;, and &lt;code&gt;mpimat&lt;/code&gt;. There is currently an experimental &lt;code&gt;parmat&lt;/code&gt; class, but it is at present not well developed. For the main classes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU: Single node cpu computing (multi-threaded if using multi-threaded BLAS and linking with OpenMP).&lt;/li&gt;
&lt;li&gt;GPU: Single gpu computing.&lt;/li&gt;
&lt;li&gt;MPI: Multi-node computing via ScaLAPACK (+gpus if using &lt;a href=&#34;http://icl.utk.edu/slate/&#34;&gt;SLATE&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some differences in how objects of any particular type are constructed. But the high level APIs are largely the same between the objects. The goal is to be able to quickly create laptop-scale prototypes that are then easily converted into large scale gpu/multi-node/multi-gpu/multi-node+multi-gpu codes.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Here&amp;rsquo;s a quick example computing the singular values of a simple CPU matrix:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;fml/cpu.hh&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;()
{
  fml&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cpumat&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; x(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
  x.fill_linspace(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;);
  
  x.info();
  x.print(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;);
  
  fml&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cpuvec&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; s;
  fml&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;svd(x, s);
  
  s.info();
  s.print();
  
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we save this in &lt;code&gt;svd.cpp&lt;/code&gt;, we can compile it with something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;g++ -I/path/to/fml/src -fopenmp svd.cpp -o svd -llapack -lblas
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And then running it, we see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cpumat 3x2 type=f
1 4 
2 5 
3 6 

# cpuvec 2 type=f
9.5080 0.7729 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With only minor modifications, we can do this same operation on a GPU:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;fml/gpu.hh&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;()
{
  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fml&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;new_card(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;);
  c.info();
  
  fml&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;gpumat&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; x(c, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
  x.fill_linspace(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;);
  
  x.info();
  x.print(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;);
  
  fml&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;gpuvec&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; s(c);
  fml&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;svd(x, s);
  
  s.info();
  s.print();
  
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we save this in &lt;code&gt;svd.cu&lt;/code&gt;, we can compile it with something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nvcc -arch=sm_61 -Xcompiler &amp;quot;-I/path/to/fml/src&amp;quot; svd.cu -o svd -lcudart -lcublas -lcusolver -lcurand -lnvidia-ml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And then running it, we see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## GPU 0 (GeForce GTX 1070 Ti) 860/8116 MB - CUDA 10.2

# gpumat 3x2 type=f 
1 4 
2 5 
3 6 

# gpuvec 2 type=f 
9.5080 0.7729 
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;the-interface&#34;&gt;The Interface&lt;/h2&gt;
&lt;p&gt;I think that one person&amp;rsquo;s high-level is another person&amp;rsquo;s low-level. With that in mind, the goal of fml is to be &amp;ldquo;medium-level&amp;rdquo;. It&amp;rsquo;s high-level compared to working directly with e.g. the BLAS or cuBLAS, but low(er)-level compared to most other C++ matrix frameworks and things like R and Matlab.&lt;/p&gt;
&lt;p&gt;If you want to learn more about the interface, you can read the &lt;a href=&#34;https://fml-fam.github.io/fml/html/index.html&#34;&gt;doxygen documentation&lt;/a&gt;. There are also some &lt;a href=&#34;https://github.com/fml-fam/fml/tree/master/examples&#34;&gt;basic examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I also maintain R bindings in the &lt;a href=&#34;https://github.com/fml-fam/fmlr&#34;&gt;fmlr&lt;/a&gt; package. That may be a good place to start. I sort of think of it like an interactive REPL for fml with some minor modifications (e.g., replace &lt;code&gt;x.method()&lt;/code&gt; with &lt;code&gt;x$method()&lt;/code&gt;). The package is &lt;a href=&#34;https://fml-fam.github.io/fmlr/html/index.html&#34;&gt;well-documented&lt;/a&gt;, and contains some long-form articles. I will have much more to say about fmlr in a later post.&lt;/p&gt;
&lt;h2 id=&#34;philosophy-and-similar-projects&#34;&gt;Philosophy and Similar Projects&lt;/h2&gt;
&lt;p&gt;There are many high quality C/C++ matrix libraries. Of course &lt;a href=&#34;http://arma.sourceforge.net/&#34;&gt;Armadillo&lt;/a&gt; and &lt;a href=&#34;http://eigen.tuxfamily.org/&#34;&gt;Eigen&lt;/a&gt; are worth mentioning. &lt;a href=&#34;http://www.boost.org/&#34;&gt;Boost&lt;/a&gt; also has matrix bindings. &lt;a href=&#34;https://www.gnu.org/software/gsl/&#34;&gt;GSL&lt;/a&gt; is a C library with numerous matrix operations. There is also &lt;a href=&#34;https://www.mcs.anl.gov/petsc/&#34;&gt;PETSc&lt;/a&gt;, which can do some distributed operations, although it is mostly focused on sparse matrices.&lt;/p&gt;
&lt;p&gt;With the exception of GSL, all of these are permissively licensed. Some have at least some level of GPU support, for example Armadillo and Eigen (perhaps others).&lt;/p&gt;
&lt;p&gt;But I believe it is fair to say that none of these projects satisfies all of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Permissively licensed.&lt;/li&gt;
&lt;li&gt;Single interface for CPU, GPU, and MPI backends (and multiple fundamental types).&lt;/li&gt;
&lt;li&gt;All functions support all backends.&lt;/li&gt;
&lt;li&gt;Only do the least amount of work necessary (e.g., QR factorization returns the compact QR and separate functions can compute Q and/or R).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That&amp;rsquo;s essentially the goal of fml. I believe this offers something legitimately new in what is already a crowded space of high-quality libraries.&lt;/p&gt;
&lt;h2 id=&#34;state-of-the-project&#34;&gt;State of the Project&lt;/h2&gt;
&lt;p&gt;The project is still pretty young, and at present only maintained/contributed to by one person, and only in my spare time. That said, it is currently usable and has quite a lot to offer if you operate at the level of linear algebra.&lt;/p&gt;
&lt;p&gt;What&amp;rsquo;s there now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most linear algebra you would want, including
&lt;ul&gt;
&lt;li&gt;Basic operations like matrix sums, products&lt;/li&gt;
&lt;li&gt;Helper operations like transpose and inversion&lt;/li&gt;
&lt;li&gt;Factorizations like SVD and QR&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-zone&#34;&gt;NVIDIA® CUDA®&lt;/a&gt; support for GPU computing&lt;/li&gt;
&lt;li&gt;First-class &lt;a href=&#34;https://github.com/fml-fam/fmlr&#34;&gt;R bindings&lt;/a&gt; developed in parallel with the core library&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What&amp;rsquo;s coming soon:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More statistics/data operations&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amd.com/en/graphics/servers-solutions-rocm&#34;&gt;AMD ROCm&lt;/a&gt; and/or hip support for GPU computing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What I would like to have some day:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More statistics/data operations than you would know what to do with&lt;/li&gt;
&lt;li&gt;Support for &lt;a href=&#34;https://icl.cs.utk.edu/magma/&#34;&gt;MAGMA&lt;/a&gt; for GPU matrix factorizations&lt;/li&gt;
&lt;li&gt;A more fleshed-out &lt;code&gt;parmat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Python bindings&lt;/li&gt;
&lt;li&gt;At least some basic in-library I/O&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SVD via Lanczos Iteration</title>
      <link>https://fml-fam.github.io/blog/2020/06/15/svd-via-lanczos-iteration/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2020/06/15/svd-via-lanczos-iteration/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Every few years, I try to figure out the Lanczos method to approximate SVD of a rectangular matrix. Unfortunately, every resource I find always leaves out enough details to confuse me. All of the information I want is available across multiple writeups, but everyone uses different notation, making things even more confusing.&lt;/p&gt;
&lt;p&gt;This time I finally sat down and got to a point where I finally felt like I understood it. This writeup will hopefully clarify how you can use the Lanczos iteration to estimate singular values/vectors of a rectangular matrix. It&amp;rsquo;s written for the kind of person who is interested in implementing it in software. If you want more information or mathematical proofs, I cite the important bits in the footnotes. Anything uncited is probably answered in &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;I assume you know what singular/eigen-values/vectors are. If you don&amp;rsquo;t know why someone would care about computing approximations of them cheaply, a good applications is truncated PCA. If you just take (approximations of) the first few rotations, then you can visualize your data in 2 or 3 dimensions. And if your dataset is big, that can be a valuable time saver.&lt;/p&gt;
&lt;p&gt;Throughout, we will use the following notation. For a vector $v$, let $\lVert v \rVert_2$ denote the Euclidean norm $\sqrt{\sum v_i^2}$. Whenever we refer to the norm of a vector, we will take that to be the Euclidean norm.&lt;/p&gt;
&lt;p&gt;We can &amp;ldquo;square up&amp;rdquo; any rectangular matrix $A$ with dimension $m\times n$ by padding with zeros to create the square, symmetric matrix $H$ with dimension $(m+n)\times (m+n)$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
H =
\begin{bmatrix}
0 &amp;amp; A \\&lt;br&gt;
A^T &amp;amp; 0
\end{bmatrix}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Finally, given scalars $\alpha_1, \dots, \alpha_k$ and $\beta_1, \dots, \beta_{k-1}$, we can form the tri-diagonal matrix $T_k$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
T_k =
\begin{bmatrix}
\alpha_1 &amp;amp; \beta_1 &amp;amp; 0 &amp;amp; \dots &amp;amp; 0 &amp;amp; 0 \\&lt;br&gt;
\beta_1 &amp;amp; \alpha_2 &amp;amp; \beta_2 &amp;amp; \dots &amp;amp; 0 &amp;amp; 0 \\&lt;br&gt;
0 &amp;amp; \beta_2 &amp;amp; \alpha_3 &amp;amp; \dots &amp;amp; 0 &amp;amp; 0 \\&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\&lt;br&gt;
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \dots &amp;amp; \alpha_{k-1} &amp;amp; \beta_{k-1} \\&lt;br&gt;
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \dots &amp;amp; \beta_{k-1} &amp;amp; \alpha_k
\end{bmatrix}
\end{align*}
$$&lt;/p&gt;
&lt;h2 id=&#34;lanczos-iteration&#34;&gt;Lanczos Iteration&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;re going to define the Lanczos iteration (Algorithm 36.1 of &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the abstract. Its mathematical motivation is probably too complicated for anyone who would find this writeup helpful. For now, we will just think of it as a process that may have some application in the future.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inputs:
&lt;ul&gt;
&lt;li&gt;Square, real, symmetric matrix $A$ of dimension $n\times n$&lt;/li&gt;
&lt;li&gt;Integer $1\leq k \leq n$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Outputs:
&lt;ul&gt;
&lt;li&gt;$k$-length vector $\alpha = \left[ \alpha_1, \alpha_2, \dots, \alpha_k \right]^T$&lt;/li&gt;
&lt;li&gt;$k$-length vector $\beta = \left[ \beta_1, \beta_2, \dots, \beta_k \right]^T$&lt;/li&gt;
&lt;li&gt;$n\times k$-dimensional matrix $Q_k = \left[ q_1, q_2, \dots, q_k \right]$ (the $q_i$ are column vectors).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Initialize $q_1$ to a random vector with norm 1. For the sake of notational convenience, treat $\beta_0=0$ and $q_0$ to be an $n$-length vector of zeros.&lt;/li&gt;
&lt;li&gt;For $i = 1, 2, \dots, k$:
&lt;ul&gt;
&lt;li&gt;$v = Aq_i$&lt;/li&gt;
&lt;li&gt;$\alpha_i = q^T_i v$&lt;/li&gt;
&lt;li&gt;$v = v - \beta_{i-1}q_{i-1} - \alpha_i q_i$&lt;/li&gt;
&lt;li&gt;$\beta_i = \lVert v \rVert_2$&lt;/li&gt;
&lt;li&gt;$q_{i+1} = \frac{1}{\beta_i}v$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In R, this might look like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;l2norm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(x) &lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x))

lanczos &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(A, k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
{
  n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(A)
  
  alpha &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;numeric&lt;/span&gt;(k)
  beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;numeric&lt;/span&gt;(k)
  
  q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, n, k)
  q[, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;runif&lt;/span&gt;(n)
  q[, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q[, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;l2norm&lt;/span&gt;(q[, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(i in &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;k)
  {
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; q[, i]
    alpha[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;crossprod&lt;/span&gt;(q[, i], v)
    
    &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
      v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; alpha[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;q[, i]
    else
      v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; beta[i&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;q[, i&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; alpha[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;q[, i]
    
    beta[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;l2norm&lt;/span&gt;(v)
    
    &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(i&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;k)
      q[, i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;beta[i]
  }
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;alpha, beta&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;beta, q&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;q)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As presented, this has nothing to do with calculating eigen-and/or-singular values/vectors. That is the subject of the next section.&lt;/p&gt;
&lt;h2 id=&#34;application-to-spectral-decomposition-and-svd&#34;&gt;Application to Spectral Decomposition and SVD&lt;/h2&gt;
&lt;p&gt;If $A$ is a square, symmetric matrix of order $n$, then you can estimate its &lt;strong&gt;eigenvalues&lt;/strong&gt; by applying the lanczos method. The $\alpha$ and $\beta$ values can be used to form the tridiagonal matrix $T_k$, and the eigenvalues of $T_k$ approximate the eigenvalues of $A$ (Theorem 12.5 of &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;). For the &lt;strong&gt;eigenvectors&lt;/strong&gt;, let $S_k$ be the eigenvectors of $T_k$. Then approximations to the eigenvectors are given by $Q_k S_k$ (Theorem 12.6 of &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;). In practice with $k\ll n$ the $T_k$ will be small enough that explicitly forming it and using a symmetric eigensolver like LAPACK&amp;rsquo;s &lt;code&gt;Xsyevr()&lt;/code&gt; or &lt;code&gt;Xsyevd()&lt;/code&gt; will suffice. You could also use &lt;code&gt;Xsteqr()&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;To calculate the &lt;strong&gt;SVD&lt;/strong&gt; of a rectangular matrix $A$ of dimension $m\times n$ and $m&amp;gt;n$, you square it up to the matrix $H$. With $H$, you now have a square, symmetric matrix of order $m+n$, so you can perform the Lanczos iteration $k$ times to approximate the eivenvalues of $H$ by the above. Note that for full convergence, we need to run the iteration $m+n$ times, not $n$ (which would be a very expensive way of computing them). The approximations to the &lt;strong&gt;singular values&lt;/strong&gt; of $A$ are given by the non-zero eigenvalues of $H$. On the other hand, the &lt;strong&gt;singular vectors&lt;/strong&gt; (left and right) are found in $Y_k := \sqrt{2}\thinspace Q_k S_k$ (Theorem 3.3.4 of &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;). Specifically, the first $m$ rows of $Y_k$ are the left singular vectors, and the remaining $n$ rows are the right singular vectors (note: not their transpose).&lt;/p&gt;
&lt;p&gt;Since the involvement of the input matrix $A$ in computing the Lanczos iteration is in the matrix-vector product $Hq_i$, it&amp;rsquo;s possible to avoid explicitly forming the squard up matrix $H$, or even to apply this to sparse problems, a common application of the algorithm. For similar reasons, this makes it very simple (or as simple as these things can be) to use it in &lt;a href=&#34;https://en.wikipedia.org/wiki/External_memory_algorithm&#34;&gt;out-of-core algorithms&lt;/a&gt; (although not &lt;a href=&#34;https://en.wikipedia.org/wiki/Online_algorithm&#34;&gt;online&lt;/a&gt; variants, given the iterative nature). Finally, note that if you do not need the eigen/singular vectors, then you do not need to store all column vectors $q_i$ of $Q_k$.&lt;/p&gt;
&lt;p&gt;If you have $m&amp;lt;n$ then I think you would just use the usual &amp;ldquo;transpose arithmetic&amp;rdquo;, although maybe even the above is fine. I haven&amp;rsquo;t thought about it and at this point I don&amp;rsquo;t care.&lt;/p&gt;
&lt;h2 id=&#34;key-takeaways-and-example-code&#34;&gt;Key Takeaways and Example Code&lt;/h2&gt;
&lt;p&gt;tldr&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spectral Decomposition
&lt;ul&gt;
&lt;li&gt;Let $A$ be a real-valued, square, symmetric matrix of order $n$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eigenvalues&lt;/strong&gt;: The non-zero eigenvalues of $T_k$ (formed by Lanczos iteration on the matrix $A$) for $k\leq n$ are approximations of the corresponding (ordered greatest to least) eigenvalues values of $A$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eigenvectors&lt;/strong&gt;: If $S_k$ are the eigenvectors of $T_k$, then the eigenvectors of $A$ are approximated by $Q_k S_k$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SVD
&lt;ul&gt;
&lt;li&gt;Let $A$ be a real-valued matrix with dimension $m\times n$ and $m&amp;lt;n$. Let $H$ be the matrix formed by &amp;ldquo;squaring up&amp;rdquo; $A$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singular values&lt;/strong&gt;: The non-zero eigenvalues of $T_k$ (formed by Lanczos iteration on the matrix $H$) for $k\leq m+n$ are approximations of the corresponding (ordered greatest to least) singular values values of $A$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singular vectors&lt;/strong&gt;: If $S_k$ are the eigenvectors of $T_k$, then $\sqrt{2}\thinspace Q_k S_k$ contain approximations to the left (first $m$ rows) and right (last $n$ rows) singular vectors.&lt;/li&gt;
&lt;li&gt;If $H$ is full rank, as $k\rightarrow m+n$ (not $n$), then the approximation becomes more accurate (if calculated in exact arithmetic).&lt;/li&gt;
&lt;li&gt;Experimentally I find that using twice the number of Lanczos iterations as desired singular values seems to work well. Examination of the error analysis (Theorem 12.7 of &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;) may reveal why, but I haven&amp;rsquo;t thought about it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ignoring some of the performance/memory concerns addressed above, we might implement this in R like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;square_up &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(x)
{
  m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(x)
  n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ncol&lt;/span&gt;(x)
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;rbind&lt;/span&gt;(
    &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;matrix&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, m, m), x),
    &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;t&lt;/span&gt;(x), &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, n, n))
  )
}

tridiagonal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(alpha, beta)
{
  n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;(alpha)
  td &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(alpha)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(i in &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;(n&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;))
  {
    td[i, i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; beta[i]
    td[i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;, i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; beta[i]
  }
  
  td
}

&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; @param A rectangular, numeric matrix with `nrow(A) &amp;gt; ncol(A)`&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; @param k The number of singular values/vectors to estimate. The number&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; of lanczos iterations is taken to be twice this number. Should be&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; &amp;#34;small&amp;#34;.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; @param only.values Should only values be returned, or also vectors?&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; @return Estimates of the first `k` singular values/vectors. Returns&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; the usual (for R) list of elements `d`, `u`, `v`.&lt;/span&gt;
lanczos_svd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(A, k, only.values&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;)
{
  m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(A)
  n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ncol&lt;/span&gt;(A)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n)
    &lt;span style=&#34;color:#a6e22e&#34;&gt;stop&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;only implemented for m&amp;gt;n&amp;#34;&lt;/span&gt;)
  
  nc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;min&lt;/span&gt;(n, k)
  
  kl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;k &lt;span style=&#34;color:#75715e&#34;&gt;# double the lanczos iterations&lt;/span&gt;
  
  H &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;square_up&lt;/span&gt;(A)
  lz &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lanczos&lt;/span&gt;(H, k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kl)
  T_kl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tridiagonal&lt;/span&gt;(lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;alpha, lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;beta)
  
  ev &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;eigen&lt;/span&gt;(T_kl, symmetric&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;, only.values&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;only.values)
  d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ev&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;values[1&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;nc]
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;only.values)
  {
    UV &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;q &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; ev&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;vectors)
    u &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; UV[1&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;m, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;nc]
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; UV&lt;span style=&#34;color:#a6e22e&#34;&gt;[&lt;/span&gt;(m&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;(m&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;n), &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;nc]
  }
  else
  {
    u &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;
  }
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(d&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;d, u&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;u, v&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;v)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And here&amp;rsquo;s a simple demonstration&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt;)
m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;
n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;rnorm&lt;/span&gt;(m&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n), m, n)

k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
lz &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lanczos_svd&lt;/span&gt;(x, k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;k, &lt;span style=&#34;color:#66d9ef&#34;&gt;FALSE&lt;/span&gt;)
sv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;svd&lt;/span&gt;(x)

&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;d, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[1] 18.43443 15.83699  0.00160
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(sv&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;d[1&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;k], &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[1] 19.69018 18.65107 18.41093
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;firstfew &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;v[firstfew, ], &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;        [,1]    [,2]     [,3]
[1,] 0.40748 0.02210 -0.00228
[2,] 0.21097 0.18573 -0.00051
[3,] 0.13179 0.01563 -0.00411
[4,] 0.39096 0.27893 -0.00117
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(sv&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;v[firstfew, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;k], &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;        [,1]     [,2]     [,3]
[1,] 0.44829 -0.28028 -0.06481
[2,] 0.18420  0.00095  0.67200
[3,] 0.07991  0.01054 -0.14656
[4,] 0.16961 -0.27779 -0.37683
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Golub, Gene H., and Charles F. Van Loan. &lt;em&gt;Matrix computations&lt;/em&gt;. Vol. 3. JHU press, 2012. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Trefethen, Lloyd N., and David Bau III. &lt;em&gt;&lt;a href=&#34;https://www.cs.cmu.edu/afs/cs/academic/class/15859n-f16/Handouts/TrefethenBau/LanczosIteration-36.pdf&#34;&gt;Numerical linear algebra&lt;/a&gt;&lt;/em&gt;. Vol. 50. Siam, 1997. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Caramanis and Sanghavi, &lt;em&gt;&lt;a href=&#34;http://users.ece.utexas.edu/~sanghavi/courses/scribed_notes/Lecture_13_and_14_Scribe_Notes.pdf&#34;&gt;Large scale learning: lecture 12&lt;/a&gt;&lt;/em&gt;. (&lt;a href=&#34;https://web.archive.org/web/20171215080524/http://users.ece.utexas.edu/~sanghavi/courses/scribed_notes/Lecture_13_and_14_Scribe_Notes.pdf&#34;&gt;archive.org link&lt;/a&gt;) &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Demmel, James W. &lt;em&gt;&lt;a href=&#34;https://books.google.com/books?hl=en&amp;amp;lr=&amp;amp;id=P3bPAgAAQBAJ&amp;amp;oi=fnd&amp;amp;pg=PR9&amp;amp;dq=applied+numerical+linear+algebra+demmel&amp;amp;ots=I7OxKaWh-y&amp;amp;sig=QKRZPGe0SiuBstzxYCg4j35gctE#v=onepage&amp;amp;q=applied%20numerical%20linear%20algebra%20demmel&amp;amp;f=false&#34;&gt;Applied numerical linear algebra&lt;/a&gt;&lt;/em&gt;. Vol. 56. Siam, 1997. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
