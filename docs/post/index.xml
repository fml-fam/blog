<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on fml blog</title>
    <link>/post/</link>
    <description>Recent content in Posts on fml blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Mon, 15 Jun 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>SVD via Lanczos Iteration</title>
      <link>/2020/06/15/svd-via-lanczos-iteration/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/15/svd-via-lanczos-iteration/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Every few years, I try to figure out the Lanczos method to approximate SVD of a rectangular matrix. Unfortunately, every resource I find always leaves out enough details to confuse me. All of the information I want is available across multiple writeups, but everyone uses different notation, making things even more confusing.&lt;/p&gt;
&lt;p&gt;This time I finally sat down and got to a point where I finally felt like I understood it. This writeup will hopefully clarify how you can use the Lanczos iteration to estimate singular values/vectors of a rectangular matrix. It&amp;rsquo;s written for the kind of person who is interested in implementing it in software. If you want more information or mathematical proofs, I cite the important bits in the footnotes. Anything uncited is probably answered in &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;I assume you know what singular/eigen-values/vectors are. If you don&amp;rsquo;t know why someone would care about computing approximations of them cheaply, a good applications is truncated PCA. If you just take (approximations of) the first few rotations, then you can visualize your data in 2 or 3 dimensions. And if your dataset is big, that can be a valuable time saver.&lt;/p&gt;
&lt;p&gt;Throughout, we will use the following notation. For a vector $v$, let $\lVert v \rVert_2$ denote the Euclidean norm $\sqrt{\sum v_i^2}$. Whenever we refer to the norm of a vector, we will take that to be the Euclidean norm.&lt;/p&gt;
&lt;p&gt;We can &amp;ldquo;square up&amp;rdquo; any rectangular matrix $A$ with dimension $m\times n$ by padding with zeros to create the square, symmetric matrix $H$ with dimension $(m+n)\times (m+n)$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
H =
\begin{bmatrix}
0 &amp;amp; A \\&lt;br&gt;
A^T &amp;amp; 0
\end{bmatrix}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Finally, given scalars $\alpha_1, \dots, \alpha_k$ and $\beta_1, \dots, \beta_{k-1}$, we can form the tri-diagonal matrix $T_k$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
T_k =
\begin{bmatrix}
\alpha_1 &amp;amp; \beta_1 &amp;amp; 0 &amp;amp; \dots &amp;amp; 0 &amp;amp; 0 \\&lt;br&gt;
\beta_1 &amp;amp; \alpha_2 &amp;amp; \beta_2 &amp;amp; \dots &amp;amp; 0 &amp;amp; 0 \\&lt;br&gt;
0 &amp;amp; \beta_2 &amp;amp; \alpha_3 &amp;amp; \dots &amp;amp; 0 &amp;amp; 0 \\&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\&lt;br&gt;
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \dots &amp;amp; \alpha_{k-1} &amp;amp; \beta_{k-1} \\&lt;br&gt;
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \dots &amp;amp; \beta_{k-1} &amp;amp; \alpha_k
\end{bmatrix}
\end{align*}
$$&lt;/p&gt;
&lt;h2 id=&#34;lanczos-iteration&#34;&gt;Lanczos Iteration&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;re going to define the Lanczos iteration (Algorithm 36.1 of &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) in the abstract. Its mathematical motivation is probably too complicated for anyone who would find this writeup helpful. For now, we will just think of it as a process that may have some application in the future.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inputs:
&lt;ul&gt;
&lt;li&gt;Square, real, symmetric matrix $A$ of dimension $n\times n$&lt;/li&gt;
&lt;li&gt;Integer $1\leq k \leq n$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Outputs:
&lt;ul&gt;
&lt;li&gt;$k$-length vector $\alpha = \left[ \alpha_1, \alpha_2, \dots, \alpha_k \right]^T$&lt;/li&gt;
&lt;li&gt;$k$-length vector $\beta = \left[ \beta_1, \beta_2, \dots, \beta_k \right]^T$&lt;/li&gt;
&lt;li&gt;$n\times k$-dimensional matrix $Q_k = \left[ q_1, q_2, \dots, q_k \right]$ (the $q_i$ are column vectors).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Initialize $q_1$ to a random vector with norm 1. For the sake of notational convenience, treat $\beta_0=0$ and $q_0$ to be an $n$-length vector of zeros.&lt;/li&gt;
&lt;li&gt;For $i = 1, 2, \dots, k$:
&lt;ul&gt;
&lt;li&gt;$v = Aq_i$&lt;/li&gt;
&lt;li&gt;$\alpha_i = q^T_i v$&lt;/li&gt;
&lt;li&gt;$v = v - \beta_{i-1}q_{i-1} - \alpha_i q_i$&lt;/li&gt;
&lt;li&gt;$\beta_i = \lVert v \rVert_2$&lt;/li&gt;
&lt;li&gt;$q_{i+1} = \frac{1}{\beta_i}v$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In R, this might look like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;l2norm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(x) &lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x))

lanczos &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(A, k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
{
  n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(A)
  
  alpha &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;numeric&lt;/span&gt;(k)
  beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;numeric&lt;/span&gt;(k)
  
  q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, n, k)
  q[, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;runif&lt;/span&gt;(n)
  q[, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q[, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;l2norm&lt;/span&gt;(q[, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(i in &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;k)
  {
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; q[, i]
    alpha[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;crossprod&lt;/span&gt;(q[, i], v)
    
    &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
      v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; alpha[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;q[, i]
    else
      v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; beta[i&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;q[, i&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; alpha[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;q[, i]
    
    beta[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;l2norm&lt;/span&gt;(v)
    
    &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(i&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;k)
      q[, i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;beta[i]
  }
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;alpha, beta&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;beta, q&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;q)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As presented, this has nothing to do with calculating eigen-and/or-singular values/vectors. That is the subject of the next section.&lt;/p&gt;
&lt;h2 id=&#34;application-to-spectral-decomposition-and-svd&#34;&gt;Application to Spectral Decomposition and SVD&lt;/h2&gt;
&lt;p&gt;If $A$ is a square, symmetric matrix of order $n$, then you can estimate its &lt;strong&gt;eigenvalues&lt;/strong&gt; by applying the lanczos method. The $\alpha$ and $\beta$ values can be used to form the tridiagonal matrix $T_k$, and the eigenvalues of $T_k$ approximate the eigenvalues of $A$ (Theorem 12.5 of &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;). For the &lt;strong&gt;eigenvectors&lt;/strong&gt;, let $S_k$ be the eigenvectors of $T_k$. Then approximations to the eigenvectors are given by $Q_k S_k$ (Theorem 12.6 of &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;). In practice with $k\ll n$ the $T_k$ will be small enough that explicitly forming it and using a symmetric eigensolver like LAPACK&amp;rsquo;s &lt;code&gt;Xsyevr()&lt;/code&gt; or &lt;code&gt;Xsyevd()&lt;/code&gt; will suffice. You could also use &lt;code&gt;Xsteqr()&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;To calculate the &lt;strong&gt;SVD&lt;/strong&gt; of a rectangular matrix $A$ of dimension $m\times n$ and $m&amp;gt;n$, you square it up to the matrix $H$. With $H$, you now have a square, symmetric matrix of order $m+n$, so you can perform the Lanczos iteration $k$ times to approximate the eivenvalues of $H$ by the above. Note that for full convergence, we need to run the iteration $m+n$ times, not $n$ (which would be a very expensive way of computing them). The approximations to the &lt;strong&gt;singular values&lt;/strong&gt; of $A$ are given by the non-zero eigenvalues of $H$. On the other hand, the &lt;strong&gt;singular vectors&lt;/strong&gt; (left and right) are found in $Y_k := \sqrt{2}\thinspace Q_k S_k$ (Theorem 3.3.4 of &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;). Specifically, the first $m$ rows of $Y_k$ are the left singular vectors, and the remaining $n$ rows are the right singular vectors (note: not their transpose).&lt;/p&gt;
&lt;p&gt;Since the involvement of the input matrix $A$ in computing the Lanczos iteration is in the matrix-vector product $Hq_i$, it&amp;rsquo;s possible to avoid explicitly forming the squard up matrix $H$, or even to apply this to sparse problems, a common application of the algorithm. For similar reasons, this makes it very simple (or as simple as these things can be) to use it in &lt;a href=&#34;https://en.wikipedia.org/wiki/External_memory_algorithm&#34;&gt;out-of-core algorithms&lt;/a&gt; (although not &lt;a href=&#34;https://en.wikipedia.org/wiki/Online_algorithm&#34;&gt;online&lt;/a&gt; variants, given the iterative nature). Finally, note that if you do not need the eigen/singular vectors, then you do not need to store all column vectors $q_i$ of $Q_k$.&lt;/p&gt;
&lt;p&gt;If you have $m&amp;lt;n$ then I think you would just use the usual &amp;ldquo;transpose arithmetic&amp;rdquo;, although maybe even the above is fine. I haven&amp;rsquo;t thought about it and at this point I don&amp;rsquo;t care.&lt;/p&gt;
&lt;h2 id=&#34;key-takeaways-and-example-code&#34;&gt;Key Takeaways and Example Code&lt;/h2&gt;
&lt;p&gt;tldr&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spectral Decomposition
&lt;ul&gt;
&lt;li&gt;Let $A$ be a real-valued, square, symmetric matrix of order $n$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eigenvalues&lt;/strong&gt;: The non-zero eigenvalues of $T_k$ (formed by Lanczos iteration on the matrix $A$) for $k\leq n$ are approximations of the corresponding (ordered greatest to least) eigenvalues values of $A$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eigenvectors&lt;/strong&gt;: If $S_k$ are the eigenvectors of $T_k$, then the eigenvectors of $A$ are approximated by $Q_k S_k$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SVD
&lt;ul&gt;
&lt;li&gt;Let $A$ be a real-valued matrix with dimension $m\times n$ and $m&amp;lt;n$. Let $H$ be the matrix formed by &amp;ldquo;squaring up&amp;rdquo; $A$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singular values&lt;/strong&gt;: The non-zero eigenvalues of $T_k$ (formed by Lanczos iteration on the matrix $H$) for $k\leq m+n$ are approximations of the corresponding (ordered greatest to least) singular values values of $A$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singular vectors&lt;/strong&gt;: If $S_k$ are the eigenvectors of $T_k$, then $\sqrt{2}\thinspace Q_k S_k$ contain approximations to the left (first $m$ rows) and right (last $n$ rows) singular vectors.&lt;/li&gt;
&lt;li&gt;If $H$ is full rank, as $k\rightarrow m+n$ (not $n$), then the approximation becomes more accurate (if calculated in exact arithmetic).&lt;/li&gt;
&lt;li&gt;Experimentally I find that using twice the number of Lanczos iterations as desired singular values seems to work well. Examination of the error analysis (Theorem 12.7 of &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;) may reveal why, but I haven&amp;rsquo;t thought about it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ignoring some of the performance/memory concerns addressed above, we might implement this in R like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;square_up &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(x)
{
  m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(x)
  n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ncol&lt;/span&gt;(x)
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;rbind&lt;/span&gt;(
    &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;matrix&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, m, m), x),
    &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;t&lt;/span&gt;(x), &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, n, n))
  )
}

tridiagonal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(alpha, beta)
{
  n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;(alpha)
  td &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(alpha)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(i in &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;(n&lt;span style=&#34;color:#ae81ff&#34;&gt;-1&lt;/span&gt;))
  {
    td[i, i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; beta[i]
    td[i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;, i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; beta[i]
  }
  
  td
}

&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; @param A rectangular, numeric matrix with `nrow(A) &amp;gt; ncol(A)`&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; @param k The number of singular values/vectors to estimate. The number&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; of lanczos iterations is taken to be twice this number. Should be&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; &amp;#34;small&amp;#34;.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; @param only.values Should only values be returned, or also vectors?&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; @return Estimates of the first `k` singular values/vectors. Returns&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&amp;#39; the usual (for R) list of elements `d`, `u`, `v`.&lt;/span&gt;
lanczos_svd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(A, k, only.values&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;)
{
  m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(A)
  n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ncol&lt;/span&gt;(A)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n)
    &lt;span style=&#34;color:#a6e22e&#34;&gt;stop&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;only implemented for m&amp;gt;n&amp;#34;&lt;/span&gt;)
  
  nc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;min&lt;/span&gt;(n, k)
  
  kl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;k &lt;span style=&#34;color:#75715e&#34;&gt;# double the lanczos iterations&lt;/span&gt;
  
  H &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;square_up&lt;/span&gt;(A)
  lz &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lanczos&lt;/span&gt;(H, k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kl)
  T_kl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tridiagonal&lt;/span&gt;(lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;alpha, lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;beta)
  
  ev &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;eigen&lt;/span&gt;(T_kl, symmetric&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;, only.values&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;only.values)
  d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ev&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;values[1&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;nc]
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if &lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;only.values)
  {
    UV &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;q &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; ev&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;vectors)
    u &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; UV[1&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;m, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;nc]
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; UV&lt;span style=&#34;color:#a6e22e&#34;&gt;[&lt;/span&gt;(m&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;(m&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;n), &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;nc]
  }
  else
  {
    u &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;
  }
  
  &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(d&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;d, u&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;u, v&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;v)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And here&amp;rsquo;s a simple demonstration&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt;)
m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;
n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;rnorm&lt;/span&gt;(m&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n), m, n)

k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
lz &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lanczos_svd&lt;/span&gt;(x, k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;k, &lt;span style=&#34;color:#66d9ef&#34;&gt;FALSE&lt;/span&gt;)
sv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;svd&lt;/span&gt;(x)

&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;d, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[1] 18.43443 15.83699  0.00160
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(sv&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;d[1&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;k], &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[1] 19.69018 18.65107 18.41093
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;firstfew &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(lz&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;v[firstfew, ], &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;        [,1]    [,2]     [,3]
[1,] 0.40748 0.02210 -0.00228
[2,] 0.21097 0.18573 -0.00051
[3,] 0.13179 0.01563 -0.00411
[4,] 0.39096 0.27893 -0.00117
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(sv&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;v[firstfew, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;k], &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;        [,1]     [,2]     [,3]
[1,] 0.44829 -0.28028 -0.06481
[2,] 0.18420  0.00095  0.67200
[3,] 0.07991  0.01054 -0.14656
[4,] 0.16961 -0.27779 -0.37683
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Golub, Gene H., and Charles F. Van Loan. &lt;em&gt;Matrix computations&lt;/em&gt;. Vol. 3. JHU press, 2012. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Trefethen, Lloyd N., and David Bau III. &lt;em&gt;&lt;a href=&#34;https://www.cs.cmu.edu/afs/cs/academic/class/15859n-f16/Handouts/TrefethenBau/LanczosIteration-36.pdf&#34;&gt;Numerical linear algebra&lt;/a&gt;&lt;/em&gt;. Vol. 50. Siam, 1997. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Caramanis and Sanghavi, &lt;em&gt;&lt;a href=&#34;http://users.ece.utexas.edu/~sanghavi/courses/scribed_notes/Lecture_13_and_14_Scribe_Notes.pdf&#34;&gt;Large scale learning: lecture 12&lt;/a&gt;&lt;/em&gt;. (&lt;a href=&#34;https://web.archive.org/web/20171215080524/http://users.ece.utexas.edu/~sanghavi/courses/scribed_notes/Lecture_13_and_14_Scribe_Notes.pdf&#34;&gt;archive.org link&lt;/a&gt;) &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Demmel, James W. &lt;em&gt;&lt;a href=&#34;https://books.google.com/books?hl=en&amp;amp;lr=&amp;amp;id=P3bPAgAAQBAJ&amp;amp;oi=fnd&amp;amp;pg=PR9&amp;amp;dq=applied+numerical+linear+algebra+demmel&amp;amp;ots=I7OxKaWh-y&amp;amp;sig=QKRZPGe0SiuBstzxYCg4j35gctE#v=onepage&amp;amp;q=applied%20numerical%20linear%20algebra%20demmel&amp;amp;f=false&#34;&gt;Applied numerical linear algebra&lt;/a&gt;&lt;/em&gt;. Vol. 56. Siam, 1997. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
