<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fml blog</title>
    <link>https://fml-fam.github.io/blog/</link>
    <description>Recent content on fml blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Tue, 29 Jun 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://fml-fam.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Matrix Computations in Constrained Memory Environments</title>
      <link>https://fml-fam.github.io/blog/2021/06/29/matrix-computations-in-constrained-memory-environments/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2021/06/29/matrix-computations-in-constrained-memory-environments/</guid>
      <description>Many times in my professional career, a researcher has approached me for help performing some kind of matrix calculation that is getting &amp;ldquo;expensive&amp;rdquo; as their data grows. Sometimes the expense is in compute time: it&amp;rsquo;s just taking too long. Recently, someone approached me who didn&amp;rsquo;t care at all how long it took to compute something. Their problem was getting the thing to run within the RAM constraints in their particular computing environment, which they couldn&amp;rsquo;t really deviate from.</description>
    </item>
    
    <item>
      <title>Matrix Factorizations for Data Analysis</title>
      <link>https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/</link>
      <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/</guid>
      <description>Integers can be factored into products of special kinds of integers with useful properties called primes. Similarly, matrices can be factored into products of special kinds of matrices with useful properties.
Because data analysis often involves operating on numeric matrices, understanding how and why to factor matrices can be very helpful. We&amp;rsquo;re going to talk about the major factorizations, namely LU, Cholesky, QR, SVD, and Eigendecomposition. There are others, but if you master these, then you&amp;rsquo;re off to a great start.</description>
    </item>
    
    <item>
      <title>Vignette-less Articles With pkgdown</title>
      <link>https://fml-fam.github.io/blog/2020/06/23/vignette-less-articles-with-pkgdown/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2020/06/23/vignette-less-articles-with-pkgdown/</guid>
      <description>Long Boring Background You Can Skip I don&amp;rsquo;t like the R vignette system. At its core, it&amp;rsquo;s a good idea. Having long-form package documentation is a good thing, and having the ability to put in source code listings that get automatically executed during rebuilds is great. But the way it works in practice is, to me, extremely annoying. Whenever you build/test your package, the vignettes will automatically be rebuilt, even if that&amp;rsquo;s not what you want.</description>
    </item>
    
    <item>
      <title>Introducing fml - the Fused Matrix Library</title>
      <link>https://fml-fam.github.io/blog/2020/06/21/introducing-fml-the-fused-matrix-library/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2020/06/21/introducing-fml-the-fused-matrix-library/</guid>
      <description>What is fml? fml is the Fused Matrix Library, a permissively licensed, header-only C++ library for dense matrix computing. The emphasis is on real-valued matrix types (float, double, and __half) for numerical operations useful for data analysis.
The library provides 3 main classes: cpumat, gpumat, and mpimat. There is currently an experimental parmat class, but it is at present not well developed. For the main classes:
 CPU: Single node cpu computing (multi-threaded if using multi-threaded BLAS and linking with OpenMP).</description>
    </item>
    
    <item>
      <title>SVD via Lanczos Iteration</title>
      <link>https://fml-fam.github.io/blog/2020/06/15/svd-via-lanczos-iteration/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/2020/06/15/svd-via-lanczos-iteration/</guid>
      <description>Background Every few years, I try to figure out the Lanczos method to approximate SVD of a rectangular matrix. Unfortunately, every resource I find always leaves out enough details to confuse me. All of the information I want is available across multiple writeups, but everyone uses different notation, making things even more confusing.
This time I finally sat down and got to a point where I finally felt like I understood it.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://fml-fam.github.io/blog/page/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://fml-fam.github.io/blog/page/about/</guid>
      <description>This is a blog dedicated to the fml project, a C++ framework for high-performance dense matrix computing. Sometimes I also talk about math related to things I&amp;rsquo;m thinking about in fml.
I use a lot of LaTeX in posts, so enabling JavaScript for MathJax will make things look much nicer. I don&amp;rsquo;t use any kind of trackers (that I&amp;rsquo;m aware of). Currently I use GitHub Pages for hosting. The blog framework is Hugo with a slightly modified Hugo theme onetwothree.</description>
    </item>
    
  </channel>
</rss>